{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fetch dataset, engineer features"
      ],
      "metadata": {
        "id": "5K_ko7o9v-Jp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujLqqThQveWp",
        "outputId": "24ad3af1-0232-4e85-a47f-a827a7a50c78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2857150127.py:13: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  data = yf.download(ticker, start=\"2010-01-01\", end=\"2025-10-31\")\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "/tmp/ipython-input-2857150127.py:106: FutureWarning: YF.download() has changed argument auto_adjust default to True\n",
            "  vix = yf.download(\"^INDIAVIX\", start=\"2010-01-01\", end=\"2025-10-31\")['Open']\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Feature engineering complete.\n",
            "Low Vol Days: 1166 | Medium Vol Days: 1555 | High Vol Days: 1149\n",
            "\n",
            "Note: Regime-wise datasets exclude both '^INDIAVIX' and 'Volatility_Regime' columns.\n"
          ]
        }
      ],
      "source": [
        "# ============================================\n",
        "# SOLARINDS Feature Engineering with Volatility Regimes\n",
        "# ============================================\n",
        "\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# --------------------------------------------\n",
        "# 1. Download SOLARINDS stock data\n",
        "# --------------------------------------------\n",
        "ticker = \"SOLARINDS.NS\"\n",
        "data = yf.download(ticker, start=\"2010-01-01\", end=\"2025-10-31\")\n",
        "\n",
        "# Flatten MultiIndex columns (for consistency)\n",
        "data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in data.columns.values]\n",
        "\n",
        "# --------------------------------------------\n",
        "# 2. Daily percentage returns\n",
        "# --------------------------------------------\n",
        "data['Return'] = data['Close_SOLARINDS.NS'].pct_change() * 100\n",
        "\n",
        "# --------------------------------------------\n",
        "# 3. Lag features (1 to 5 days)\n",
        "# --------------------------------------------\n",
        "for lag in range(1, 6):\n",
        "    data[f'Lag{lag}'] = data['Return'].shift(lag)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 4. Volume (in billions)\n",
        "# --------------------------------------------\n",
        "data['Volume'] = data['Volume_SOLARINDS.NS'] / 1e9\n",
        "\n",
        "# --------------------------------------------\n",
        "# 5. RSI (14-day) and 3-day average RSI (using data up to t-1)\n",
        "# --------------------------------------------\n",
        "window_length = 14\n",
        "delta = data['Close_SOLARINDS.NS'].diff()\n",
        "\n",
        "gain = delta.clip(lower=0)\n",
        "loss = -delta.clip(upper=0)\n",
        "\n",
        "avg_gain = gain.rolling(window=window_length, min_periods=window_length).mean()\n",
        "avg_loss = loss.rolling(window=window_length, min_periods=window_length).mean()\n",
        "\n",
        "rs = avg_gain / avg_loss\n",
        "rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "# Shift RSI forward by one day so RSI_t uses data up to t-1\n",
        "data['RSI'] = rsi.shift(1)\n",
        "data['RSI_Avg3'] = data['RSI'].rolling(window=3).mean()\n",
        "\n",
        "# --------------------------------------------\n",
        "# 6. EMA features (20-day) — computed using data up to t-1\n",
        "# --------------------------------------------\n",
        "ema_period = 20\n",
        "data['EMA_raw'] = data['Close_SOLARINDS.NS'].ewm(span=ema_period, adjust=False).mean()\n",
        "\n",
        "# Shift EMA so that EMA_t only uses prices up to t-1\n",
        "data['EMA'] = data['EMA_raw'].shift(1)\n",
        "\n",
        "# Derived EMA-based features (compare t's close with EMA_{t-1})\n",
        "data['EMA_Compare'] = np.where(\n",
        "    data['Close_SOLARINDS.NS'] > data['EMA'],\n",
        "    'Above EMA',\n",
        "    'Below EMA'\n",
        ")\n",
        "\n",
        "data['EMA_Diff_Pct'] = ((data['Close_SOLARINDS.NS'] - data['EMA']) / data['EMA']) * 100\n",
        "\n",
        "# --------------------------------------------\n",
        "# 7. Weighted lag average (decaying weights)\n",
        "# --------------------------------------------\n",
        "weights = np.array([0.5**(i-1) for i in range(1, 6)])\n",
        "weights = weights / weights.sum()\n",
        "lag_cols = [f'Lag{i}' for i in range(1, 6)]\n",
        "data['Lag_Weighted_Avg'] = data[lag_cols].mul(weights, axis=1).sum(axis=1)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 8. Today's return and Direction classification\n",
        "# --------------------------------------------\n",
        "data['Today'] = data['Return']\n",
        "\n",
        "up_thresh = 1\n",
        "down_thresh = -1\n",
        "\n",
        "def classify_direction(x, up_thresh=up_thresh, down_thresh=down_thresh):\n",
        "    if x > up_thresh:\n",
        "        return 'UP'\n",
        "    elif x < down_thresh:\n",
        "        return 'DOWN'\n",
        "    else:\n",
        "        return 'HOLD'\n",
        "\n",
        "data['Direction'] = data['Today'].apply(classify_direction)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 9. Add Year\n",
        "# --------------------------------------------\n",
        "data['Date'] = data.index\n",
        "data['Year'] = data.index.year\n",
        "\n",
        "# --------------------------------------------\n",
        "# 10. Add India VIX and define volatility regimes\n",
        "# --------------------------------------------\n",
        "vix = yf.download(\"^INDIAVIX\", start=\"2010-01-01\", end=\"2025-10-31\")['Open']\n",
        "vix.name = '^INDIAVIX'\n",
        "data = data.merge(vix, left_index=True, right_index=True, how='left')\n",
        "\n",
        "low_thresh = data['^INDIAVIX'].quantile(0.30)\n",
        "high_thresh = data['^INDIAVIX'].quantile(0.70)\n",
        "\n",
        "def classify_volatility(v):\n",
        "    if v < low_thresh:\n",
        "        return 'Low'\n",
        "    elif v > high_thresh:\n",
        "        return 'High'\n",
        "    else:\n",
        "        return 'Medium'\n",
        "\n",
        "data['Volatility_Regime'] = data['^INDIAVIX'].apply(classify_volatility)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 11. Define final feature set (without VIX or Regime)\n",
        "# --------------------------------------------\n",
        "feature_cols = [ 'Date',\n",
        "    'Close_SOLARINDS.NS',\n",
        "    'Lag1', 'Lag2', 'Lag3', 'Lag4', 'Lag5',\n",
        "    'Lag_Weighted_Avg',\n",
        "    'Volume',\n",
        "    'RSI', 'RSI_Avg3',\n",
        "    'EMA', 'EMA_Compare', 'EMA_Diff_Pct',\n",
        "    'Year',\n",
        "    'Direction'  # target variable\n",
        "]\n",
        "\n",
        "final_cols_with_vix = feature_cols + ['^INDIAVIX', 'Volatility_Regime']\n",
        "\n",
        "# --------------------------------------------\n",
        "# 12. Create final_df with volatility classification\n",
        "# --------------------------------------------\n",
        "final_df = data[final_cols_with_vix].dropna().reset_index(drop=True)\n",
        "\n",
        "# Split by regime (but drop regime & VIX from each subset)\n",
        "low_vol_df = final_df[final_df['Volatility_Regime'] == 'Low'][feature_cols].reset_index(drop=True)\n",
        "med_vol_df = final_df[final_df['Volatility_Regime'] == 'Medium'][feature_cols].reset_index(drop=True)\n",
        "high_vol_df = final_df[final_df['Volatility_Regime'] == 'High'][feature_cols].reset_index(drop=True)\n",
        "\n",
        "# --------------------------------------------\n",
        "# 13. Save and summarize\n",
        "# --------------------------------------------\n",
        "final_df.to_csv(\"SOLARINDS_Features_with_VIX.csv\", index=False)\n",
        "low_vol_df.to_csv(\"SOLARINDS_LowVol.csv\", index=False)\n",
        "med_vol_df.to_csv(\"SOLARINDS_MedVol.csv\", index=False)\n",
        "high_vol_df.to_csv(\"SOLARINDS_HighVol.csv\", index=False)\n",
        "\n",
        "print(\"✅ Feature engineering complete.\")\n",
        "print(f\"Low Vol Days: {len(low_vol_df)} | Medium Vol Days: {len(med_vol_df)} | High Vol Days: {len(high_vol_df)}\")\n",
        "print(\"\\nNote: Regime-wise datasets exclude both '^INDIAVIX' and 'Volatility_Regime' columns.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data in tabular form"
      ],
      "metadata": {
        "id": "JqFCjeMHwmCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df['Direction'].value_counts() #find count of each class in direction\n",
        "\n",
        "#clearly, hold has significantly more datapoints than the other two classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "YC2_-ZYbL94Q",
        "outputId": "1b92e881-a6c2-4011-c527-76035f991376"
      },
      "execution_count": 235,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Direction\n",
              "HOLD    2097\n",
              "UP       928\n",
              "DOWN     845\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Direction</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>HOLD</th>\n",
              "      <td>2097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UP</th>\n",
              "      <td>928</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DOWN</th>\n",
              "      <td>845</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 235
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "fSoiRmTzwMDY",
        "outputId": "e2187982-7535-46e0-8197-b3fcc7623ff6"
      },
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Date  Close_SOLARINDS.NS      Lag1      Lag2      Lag3      Lag4  \\\n",
              "0 2010-01-28           75.416298 -2.031917 -3.128298  4.314381 -4.749748   \n",
              "1 2010-01-29           75.748512  3.703666 -2.031917 -3.128298  4.314381   \n",
              "2 2010-02-01           75.990921  0.440507  3.703666 -2.031917 -3.128298   \n",
              "3 2010-02-02           76.206390  0.320018  0.440507  3.703666 -2.031917   \n",
              "4 2010-02-03           76.565498  0.283546  0.320018  0.440507  3.703666   \n",
              "\n",
              "       Lag5  Lag_Weighted_Avg    Volume        RSI   RSI_Avg3        EMA  \\\n",
              "0 -0.716583         -1.628890  0.000046  47.551642  51.405888  75.471925   \n",
              "1 -4.749748          1.108682  0.000181  53.229872  50.796769  75.466627   \n",
              "2  4.314381          0.858309  0.000002  56.312849  52.364787  75.493474   \n",
              "3 -3.128298          0.524738  0.000004  54.622797  54.721839  75.540849   \n",
              "4 -2.031917          0.459172  0.000020  52.063752  54.333133  75.604234   \n",
              "\n",
              "  EMA_Compare  EMA_Diff_Pct  Year Direction  ^INDIAVIX Volatility_Regime  \n",
              "0   Below EMA     -0.073706  2010        UP  27.570000              High  \n",
              "1   Above EMA      0.373523  2010      HOLD  28.590000              High  \n",
              "2   Above EMA      0.658928  2010      HOLD  26.799999              High  \n",
              "3   Above EMA      0.881034  2010      HOLD  24.780001              High  \n",
              "4   Above EMA      1.271442  2010      HOLD  26.059999              High  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6017e390-226e-4a2a-afc6-9d41206d7877\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Close_SOLARINDS.NS</th>\n",
              "      <th>Lag1</th>\n",
              "      <th>Lag2</th>\n",
              "      <th>Lag3</th>\n",
              "      <th>Lag4</th>\n",
              "      <th>Lag5</th>\n",
              "      <th>Lag_Weighted_Avg</th>\n",
              "      <th>Volume</th>\n",
              "      <th>RSI</th>\n",
              "      <th>RSI_Avg3</th>\n",
              "      <th>EMA</th>\n",
              "      <th>EMA_Compare</th>\n",
              "      <th>EMA_Diff_Pct</th>\n",
              "      <th>Year</th>\n",
              "      <th>Direction</th>\n",
              "      <th>^INDIAVIX</th>\n",
              "      <th>Volatility_Regime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2010-01-28</td>\n",
              "      <td>75.416298</td>\n",
              "      <td>-2.031917</td>\n",
              "      <td>-3.128298</td>\n",
              "      <td>4.314381</td>\n",
              "      <td>-4.749748</td>\n",
              "      <td>-0.716583</td>\n",
              "      <td>-1.628890</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>47.551642</td>\n",
              "      <td>51.405888</td>\n",
              "      <td>75.471925</td>\n",
              "      <td>Below EMA</td>\n",
              "      <td>-0.073706</td>\n",
              "      <td>2010</td>\n",
              "      <td>UP</td>\n",
              "      <td>27.570000</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2010-01-29</td>\n",
              "      <td>75.748512</td>\n",
              "      <td>3.703666</td>\n",
              "      <td>-2.031917</td>\n",
              "      <td>-3.128298</td>\n",
              "      <td>4.314381</td>\n",
              "      <td>-4.749748</td>\n",
              "      <td>1.108682</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>53.229872</td>\n",
              "      <td>50.796769</td>\n",
              "      <td>75.466627</td>\n",
              "      <td>Above EMA</td>\n",
              "      <td>0.373523</td>\n",
              "      <td>2010</td>\n",
              "      <td>HOLD</td>\n",
              "      <td>28.590000</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2010-02-01</td>\n",
              "      <td>75.990921</td>\n",
              "      <td>0.440507</td>\n",
              "      <td>3.703666</td>\n",
              "      <td>-2.031917</td>\n",
              "      <td>-3.128298</td>\n",
              "      <td>4.314381</td>\n",
              "      <td>0.858309</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>56.312849</td>\n",
              "      <td>52.364787</td>\n",
              "      <td>75.493474</td>\n",
              "      <td>Above EMA</td>\n",
              "      <td>0.658928</td>\n",
              "      <td>2010</td>\n",
              "      <td>HOLD</td>\n",
              "      <td>26.799999</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2010-02-02</td>\n",
              "      <td>76.206390</td>\n",
              "      <td>0.320018</td>\n",
              "      <td>0.440507</td>\n",
              "      <td>3.703666</td>\n",
              "      <td>-2.031917</td>\n",
              "      <td>-3.128298</td>\n",
              "      <td>0.524738</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>54.622797</td>\n",
              "      <td>54.721839</td>\n",
              "      <td>75.540849</td>\n",
              "      <td>Above EMA</td>\n",
              "      <td>0.881034</td>\n",
              "      <td>2010</td>\n",
              "      <td>HOLD</td>\n",
              "      <td>24.780001</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2010-02-03</td>\n",
              "      <td>76.565498</td>\n",
              "      <td>0.283546</td>\n",
              "      <td>0.320018</td>\n",
              "      <td>0.440507</td>\n",
              "      <td>3.703666</td>\n",
              "      <td>-2.031917</td>\n",
              "      <td>0.459172</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>52.063752</td>\n",
              "      <td>54.333133</td>\n",
              "      <td>75.604234</td>\n",
              "      <td>Above EMA</td>\n",
              "      <td>1.271442</td>\n",
              "      <td>2010</td>\n",
              "      <td>HOLD</td>\n",
              "      <td>26.059999</td>\n",
              "      <td>High</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6017e390-226e-4a2a-afc6-9d41206d7877')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6017e390-226e-4a2a-afc6-9d41206d7877 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6017e390-226e-4a2a-afc6-9d41206d7877');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-750d1dc4-cc6f-47af-9725-6e4c1c226e53\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-750d1dc4-cc6f-47af-9725-6e4c1c226e53')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-750d1dc4-cc6f-47af-9725-6e4c1c226e53 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "final_df",
              "summary": "{\n  \"name\": \"final_df\",\n  \"rows\": 3870,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2010-01-28 00:00:00\",\n        \"max\": \"2025-10-30 00:00:00\",\n        \"num_unique_values\": 3870,\n        \"samples\": [\n          \"2018-11-27 00:00:00\",\n          \"2015-11-03 00:00:00\",\n          \"2020-02-26 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close_SOLARINDS.NS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3551.716598564825,\n        \"min\": 71.56242370605469,\n        \"max\": 17587.47265625,\n        \"num_unique_values\": 3783,\n        \"samples\": [\n          619.281494140625,\n          628.0958862304688,\n          16871.900390625\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lag1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.873898349360768,\n        \"min\": -11.766111888979546,\n        \"max\": 11.6225887539245,\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          -0.6230357991700686,\n          -0.8125891720968781,\n          -0.303035657804529\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lag2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.873130378748909,\n        \"min\": -11.766111888979546,\n        \"max\": 11.6225887539245,\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          0.9748551590586851,\n          1.3072689057677556,\n          -0.3498976200532855\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lag3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8762118092916193,\n        \"min\": -11.766111888979546,\n        \"max\": 11.6225887539245,\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          -0.6475793565269949,\n          2.105158526961337,\n          0.4187348851825812\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lag4\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.8799712542486748,\n        \"min\": -11.766111888979546,\n        \"max\": 11.6225887539245,\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          -1.1906709529000858,\n          -0.07012306782887556,\n          -0.4599381307315853\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lag5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.875806719777931,\n        \"min\": -11.766111888979546,\n        \"max\": 11.6225887539245,\n        \"num_unique_values\": 3858,\n        \"samples\": [\n          0.4807880334699588,\n          0.6412916124732604,\n          1.055497999457189\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Lag_Weighted_Avg\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1261002580980495,\n        \"min\": -7.258332400238849,\n        \"max\": 7.295557789308798,\n        \"num_unique_values\": 3870,\n        \"samples\": [\n          -0.4158211531243272,\n          -0.4944897873571473,\n          -1.0488457388461108\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.947220837069144e-05,\n        \"min\": 0.0,\n        \"max\": 0.003479937,\n        \"num_unique_values\": 3665,\n        \"samples\": [\n          7.748e-06,\n          3.9413e-05,\n          9.452e-06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RSI\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.195610922765386,\n        \"min\": 6.119548958692903,\n        \"max\": 98.5497436272708,\n        \"num_unique_values\": 3867,\n        \"samples\": [\n          59.647079623684625,\n          46.36436988666216,\n          36.06039937009371\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RSI_Avg3\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 16.57145168274152,\n        \"min\": 7.454949222929206,\n        \"max\": 96.47907260016162,\n        \"num_unique_values\": 3870,\n        \"samples\": [\n          55.17373643534222,\n          45.29639772426128,\n          37.47164679238999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EMA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3492.2004141781235,\n        \"min\": 74.29783433603023,\n        \"max\": 16822.5081431175,\n        \"num_unique_values\": 3870,\n        \"samples\": [\n          986.3292712373213,\n          660.5682784910758,\n          1190.053043569789\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EMA_Compare\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Above EMA\",\n          \"Below EMA\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"EMA_Diff_Pct\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4.640006531804443,\n        \"min\": -19.37384318158308,\n        \"max\": 30.287738952310956,\n        \"num_unique_values\": 3870,\n        \"samples\": [\n          1.568291618337313,\n          -2.1803022345074514\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Year\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 16,\n        \"samples\": [\n          2010,\n          2011\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Direction\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"UP\",\n          \"HOLD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"^INDIAVIX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.030538931472735,\n        \"min\": 9.890000343322754,\n        \"max\": 83.61000061035156,\n        \"num_unique_values\": 1552,\n        \"samples\": [\n          21.809999465942383,\n          21.579999923706055\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volatility_Regime\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"High\",\n          \"Medium\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Basic Softmax model"
      ],
      "metadata": {
        "id": "VmdfnitBwFX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n"
      ],
      "metadata": {
        "id": "bW6d5LuKwKpX"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "\n",
        "df = final_df.copy()\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Volatility_Regime', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "YlKtj_dZytQH"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['Direction', 'Volatility_Regime', 'Close_SOLARINDS.NS', 'Date'])\n",
        "y = df['Direction']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "VORMMr5DzASW"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(random_state=17)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "#resamples data to remove imbalance of datapoints per class as observed earlier\n",
        "\n",
        "print(\"original data: \", '\\n', y_train.value_counts(), '\\n')\n",
        "\n",
        "print(\"resampled data: \", '\\n', y_resampled.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_yG1pnRK8cJ",
        "outputId": "b536ebd3-7a5c-4b46-bb16-5fe56f8782cd"
      },
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original data:  \n",
            " Direction\n",
            "1    1678\n",
            "2     742\n",
            "0     676\n",
            "Name: count, dtype: int64 \n",
            "\n",
            "resampled data:  \n",
            " Direction\n",
            "1    1678\n",
            "2    1678\n",
            "0    1678\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_model = LogisticRegression(solver='lbfgs', penalty='l2', C=0.5, max_iter=1000)\n",
        "\n",
        "softmax_model.fit(X_resampled, y_resampled)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "id": "fbad_jnzzOqD",
        "outputId": "4639f8ba-11d6-403e-fcf6-3fb883da331b"
      },
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=0.5, max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-5 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: #000;\n",
              "  --sklearn-color-text-muted: #666;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-5 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-5 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-5 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-5 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-5 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: flex;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "  align-items: start;\n",
              "  justify-content: space-between;\n",
              "  gap: 0.5em;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 label.sk-toggleable__label .caption {\n",
              "  font-size: 0.6rem;\n",
              "  font-weight: lighter;\n",
              "  color: var(--sklearn-color-text-muted);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-5 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-5 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-5 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-5 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-5 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-5 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-5 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 0.5em;\n",
              "  text-align: center;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-5 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-5 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-5 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(C=0.5, max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.5, max_iter=1000)</pre></div> </div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = softmax_model.predict(X_test)"
      ],
      "metadata": {
        "id": "5TwoiDiOzapG"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model Evaluation:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))\n",
        "\n",
        "# confusion matrix:\n",
        "# rows = true class\n",
        "# columns = predicted class\n",
        "# Cij = no of datapoints of class i predicted as class j"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YbFqMFWyN3F",
        "outputId": "863b6bd0-1825-4757-e371-d15dbb53cb52"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Evaluation:\n",
            "[[130  32   7]\n",
            " [102 240  77]\n",
            " [  4  38 144]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.55      0.77      0.64       169\n",
            "        HOLD       0.77      0.57      0.66       419\n",
            "          UP       0.63      0.77      0.70       186\n",
            "\n",
            "    accuracy                           0.66       774\n",
            "   macro avg       0.65      0.71      0.67       774\n",
            "weighted avg       0.69      0.66      0.66       774\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Separate models for high, medium and low market volatility"
      ],
      "metadata": {
        "id": "4LG02IA7IYz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High Volatility"
      ],
      "metadata": {
        "id": "mkkQmYumPPQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# --- Step 1: Copy original dataframe ---\n",
        "hdf = high_vol_df.copy()\n",
        "\n",
        "# --- Step 2: Ensure Date column is datetime ---\n",
        "hdf['Date'] = pd.to_datetime(hdf['Date'])\n",
        "\n",
        "# --- Step 3: Encode categorical columns ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    hdf[col] = le.fit_transform(hdf[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 4: Split into train (2010–2019) and test (2020–2025) ---\n",
        "train_df = hdf[hdf['Date'].dt.year <= 2019].copy()\n",
        "test_df  = hdf[hdf['Date'].dt.year >= 2020].copy()\n",
        "\n",
        "# --- Step 5: Separate features and target ---\n",
        "X_train = train_df.drop(columns=['Direction', 'Close_SOLARINDS.NS'])\n",
        "y_train = train_df['Direction']\n",
        "X_test  = test_df.drop(columns=['Direction', 'Close_SOLARINDS.NS'])\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# --- Step 6: Scale numeric features (fit on train, transform on test) ---\n",
        "scaler = StandardScaler()\n",
        "X_train_features = X_train.drop(columns=['Date', 'Year'], errors='ignore')\n",
        "X_test_features  = X_test.drop(columns=['Date', 'Year'], errors='ignore')\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "X_test_scaled  = scaler.transform(X_test_features)\n",
        "\n",
        "# --- Step 7: Handle imbalance using SMOTE ---\n",
        "smote = SMOTE(random_state=17)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 8: Train softmax logistic regression ---\n",
        "softmax_model = LogisticRegression(solver='lbfgs', penalty='l2', C=0.5, max_iter=1000)\n",
        "softmax_model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# --- Feature Importance (Coefficient Display) ---\n",
        "# Use feature names from X_train_features for display\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train_features.columns,\n",
        "    'Coefficient': softmax_model.coef_[0]\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "print(\"\\nFeature importances (coefficients):\")\n",
        "print(importance_df)\n",
        "\n",
        "# --- Step 9: Predictions & probability scores ---\n",
        "y_pred = softmax_model.predict(X_test_scaled)\n",
        "y_proba = softmax_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 10: Build probability dataframe ---\n",
        "proba_df_high = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in softmax_model.classes_]\n",
        ")\n",
        "proba_df_high[\"Predicted\"] = y_pred\n",
        "proba_df_high[\"Confidence\"] = proba_df_high[[f\"P_{cls}\" for cls in softmax_model.classes_]].max(axis=1)\n",
        "\n",
        "# Add back corresponding Date and Close price for backtesting\n",
        "proba_df_high[\"Date\"] = test_df[\"Date\"].values\n",
        "proba_df_high[\"Close_SOLARINDS.NS\"] = test_df[\"Close_SOLARINDS.NS\"].values\n",
        "\n",
        "# --- Step 11: Save or preview ---\n",
        "proba_df_high.to_csv(\"probability_scores_high.csv\", index=False)\n",
        "print(\"\\nProbability scores (first 10 samples):\")\n",
        "print(proba_df_high.head(10))\n",
        "\n",
        "# --- Step 12: Evaluation ---\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))\n",
        "\n"
      ],
      "metadata": {
        "id": "6mS0VHlSICQz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14092cd4-f5c6-4bd1-82d0-01c4c59f7b15"
      },
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature importances (coefficients):\n",
            "             Feature  Coefficient\n",
            "8           RSI_Avg3     1.712992\n",
            "5   Lag_Weighted_Avg     0.949646\n",
            "0               Lag1     0.635319\n",
            "3               Lag4     0.571628\n",
            "2               Lag3     0.559055\n",
            "4               Lag5     0.475628\n",
            "10       EMA_Compare     0.467750\n",
            "1               Lag2     0.405055\n",
            "7                RSI     0.166844\n",
            "6             Volume    -0.029813\n",
            "9                EMA    -0.216796\n",
            "11      EMA_Diff_Pct    -3.641305\n",
            "\n",
            "Probability scores (first 10 samples):\n",
            "        P_0       P_1           P_2  Predicted  Confidence       Date  \\\n",
            "0  0.057160  0.611346  3.314935e-01          1    0.611346 2020-03-02   \n",
            "1  0.077287  0.290879  6.318349e-01          2    0.631835 2020-03-03   \n",
            "2  0.212261  0.588369  1.993703e-01          1    0.588369 2020-03-04   \n",
            "3  0.035012  0.492571  4.724172e-01          1    0.492571 2020-03-05   \n",
            "4  0.449297  0.498428  5.227491e-02          1    0.498428 2020-03-06   \n",
            "5  0.781256  0.212834  5.910301e-03          0    0.781256 2020-03-09   \n",
            "6  0.352800  0.592608  5.459209e-02          1    0.592608 2020-03-11   \n",
            "7  0.999332  0.000668  9.884798e-08          0    0.999332 2020-03-12   \n",
            "8  0.003043  0.251828  7.451283e-01          2    0.745128 2020-03-13   \n",
            "9  0.961228  0.038097  6.745384e-04          0    0.961228 2020-03-16   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1130.145874  \n",
            "1         1122.120483  \n",
            "2         1112.962646  \n",
            "3         1121.578857  \n",
            "4         1115.178345  \n",
            "5         1095.878174  \n",
            "6         1101.884888  \n",
            "7         1021.188171  \n",
            "8         1055.308228  \n",
            "9         1023.797607  \n",
            "\n",
            "Model Evaluation:\n",
            "[[ 40  54   4]\n",
            " [ 18 156  38]\n",
            " [  2  25  83]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.67      0.41      0.51        98\n",
            "        HOLD       0.66      0.74      0.70       212\n",
            "          UP       0.66      0.75      0.71       110\n",
            "\n",
            "    accuracy                           0.66       420\n",
            "   macro avg       0.66      0.63      0.64       420\n",
            "weighted avg       0.66      0.66      0.66       420\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medium Volatility"
      ],
      "metadata": {
        "id": "pElIMJqSQ7qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# --- Step 1: Copy original dataframe ---\n",
        "mdf = med_vol_df.copy()\n",
        "\n",
        "# --- Step 2: Ensure Date column is datetime ---\n",
        "mdf['Date'] = pd.to_datetime(mdf['Date'])\n",
        "\n",
        "# --- Step 3: Encode categorical columns ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    mdf[col] = le.fit_transform(mdf[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 4: Split into train (2010–2019) and test (2020–2025) ---\n",
        "train_df = mdf[mdf['Date'].dt.year <= 2019].copy()\n",
        "test_df  = mdf[mdf['Date'].dt.year >= 2020].copy()\n",
        "\n",
        "# --- Step 5: Separate features and target ---\n",
        "X_train = train_df.drop(columns=['Direction', 'Close_SOLARINDS.NS'])\n",
        "y_train = train_df['Direction']\n",
        "X_test  = test_df.drop(columns=['Direction', 'Close_SOLARINDS.NS'])\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# --- Step 6: Scale numeric features (fit on train, transform on test) ---\n",
        "scaler = StandardScaler()\n",
        "X_train_features = X_train.drop(columns=['Date', 'Year'], errors='ignore')\n",
        "X_test_features  = X_test.drop(columns=['Date', 'Year'], errors='ignore')\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "X_test_scaled  = scaler.transform(X_test_features)\n",
        "\n",
        "# --- Step 7: Handle imbalance using SMOTE ---\n",
        "smote = SMOTE(random_state=17)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 8: Train softmax logistic regression ---\n",
        "softmax_model = LogisticRegression(solver='lbfgs', penalty='l2', C=0.5, max_iter=1000)\n",
        "softmax_model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# --- Feature Importance (Coefficient Display) ---\n",
        "# Use feature names from X_train_features for display\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train_features.columns,\n",
        "    'Coefficient': softmax_model.coef_[0]\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "print(\"\\nFeature importances (coefficients):\")\n",
        "print(importance_df)\n",
        "\n",
        "# --- Step 9: Predictions & probability scores ---\n",
        "y_pred = softmax_model.predict(X_test_scaled)\n",
        "y_proba = softmax_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 10: Build probability dataframe ---\n",
        "proba_df_med = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in softmax_model.classes_]\n",
        ")\n",
        "proba_df_med[\"Predicted\"] = y_pred\n",
        "proba_df_med[\"Confidence\"] = proba_df_med[[f\"P_{cls}\" for cls in softmax_model.classes_]].max(axis=1)\n",
        "\n",
        "# Add back corresponding Date and Close price for backtesting\n",
        "proba_df_med[\"Date\"] = test_df[\"Date\"].values\n",
        "proba_df_med[\"Close_SOLARINDS.NS\"] = test_df[\"Close_SOLARINDS.NS\"].values\n",
        "\n",
        "# --- Step 11: Save or preview ---\n",
        "proba_df_med.to_csv(\"probability_scores_med.csv\", index=False)\n",
        "print(\"\\nProbability scores (first 10 samples):\")\n",
        "print(proba_df_med.head(10))\n",
        "\n",
        "# --- Step 12: Evaluation ---\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))\n",
        "\n"
      ],
      "metadata": {
        "id": "Xf-uuRZmRA5T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373e63d7-ac4a-48a4-d863-aa96f4f5564c"
      },
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature importances (coefficients):\n",
            "             Feature  Coefficient\n",
            "8           RSI_Avg3     1.577920\n",
            "5   Lag_Weighted_Avg     0.922041\n",
            "1               Lag2     0.608133\n",
            "0               Lag1     0.564803\n",
            "2               Lag3     0.507122\n",
            "10       EMA_Compare     0.461434\n",
            "3               Lag4     0.425299\n",
            "7                RSI     0.422333\n",
            "4               Lag5     0.344669\n",
            "9                EMA     0.074706\n",
            "6             Volume    -0.050400\n",
            "11      EMA_Diff_Pct    -4.022673\n",
            "\n",
            "Probability scores (first 10 samples):\n",
            "        P_0       P_1       P_2  Predicted  Confidence       Date  \\\n",
            "0  0.840056  0.156285  0.003659          0    0.840056 2020-01-07   \n",
            "1  0.753697  0.234266  0.012037          0    0.753697 2020-01-08   \n",
            "2  0.332413  0.569337  0.098250          1    0.569337 2020-01-09   \n",
            "3  0.271537  0.634296  0.094168          1    0.634296 2020-01-21   \n",
            "4  0.582086  0.397075  0.020839          0    0.582086 2020-01-22   \n",
            "5  0.659553  0.319527  0.020920          0    0.659553 2020-01-23   \n",
            "6  0.006411  0.202284  0.791305          2    0.791305 2020-01-24   \n",
            "7  0.029884  0.549113  0.421003          1    0.549113 2020-01-27   \n",
            "8  0.011675  0.481315  0.507011          2    0.507011 2020-01-28   \n",
            "9  0.000575  0.163059  0.836367          2    0.836367 2020-01-29   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1089.772949  \n",
            "1         1087.114258  \n",
            "2         1094.548706  \n",
            "3         1143.094727  \n",
            "4         1143.882568  \n",
            "5         1133.395386  \n",
            "6         1161.902588  \n",
            "7         1179.479492  \n",
            "8         1204.687988  \n",
            "9         1253.381592  \n",
            "\n",
            "Model Evaluation:\n",
            "[[ 94  12   6]\n",
            " [ 87  24  70]\n",
            " [ 19  11 102]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.47      0.84      0.60       112\n",
            "        HOLD       0.51      0.13      0.21       181\n",
            "          UP       0.57      0.77      0.66       132\n",
            "\n",
            "    accuracy                           0.52       425\n",
            "   macro avg       0.52      0.58      0.49       425\n",
            "weighted avg       0.52      0.52      0.45       425\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low Volatility"
      ],
      "metadata": {
        "id": "gnArKULrRB-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# --- Step 1: Copy original dataframe ---\n",
        "ldf = low_vol_df.copy()\n",
        "\n",
        "# --- Step 2: Ensure Date column is datetime ---\n",
        "ldf['Date'] = pd.to_datetime(ldf['Date'])\n",
        "\n",
        "# --- Step 3: Encode categorical columns ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    ldf[col] = le.fit_transform(ldf[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 4: Split into train (2010–2019) and test (2020–2025) ---\n",
        "train_df = ldf[ldf['Date'].dt.year <= 2019].copy()\n",
        "test_df  = ldf[ldf['Date'].dt.year >= 2020].copy()\n",
        "\n",
        "# --- Step 5: Separate features and target ---\n",
        "X_train = train_df.drop(columns=['Direction', 'Close_SOLARINDS.NS'])\n",
        "y_train = train_df['Direction']\n",
        "X_test  = test_df.drop(columns=['Direction', 'Close_SOLARINDS.NS'])\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# --- Step 6: Scale numeric features (fit on train, transform on test) ---\n",
        "scaler = StandardScaler()\n",
        "X_train_features = X_train.drop(columns=['Date', 'Year'], errors='ignore')\n",
        "X_test_features  = X_test.drop(columns=['Date', 'Year'], errors='ignore')\n",
        "X_train_scaled = scaler.fit_transform(X_train_features)\n",
        "X_test_scaled  = scaler.transform(X_test_features)\n",
        "\n",
        "# --- Step 7: Handle imbalance using SMOTE ---\n",
        "smote = SMOTE(random_state=17)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 8: Train softmax logistic regression ---\n",
        "softmax_model = LogisticRegression(solver='lbfgs', penalty='l2', C=0.5, max_iter=1000)\n",
        "softmax_model.fit(X_resampled, y_resampled)\n",
        "\n",
        "# --- Feature Importance (Coefficient Display) ---\n",
        "# Use feature names from X_train_features for display\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train_features.columns,\n",
        "    'Coefficient': softmax_model.coef_[0]\n",
        "}).sort_values(by='Coefficient', ascending=False)\n",
        "print(\"\\nFeature importances (coefficients):\")\n",
        "print(importance_df)\n",
        "\n",
        "# --- Step 9: Predictions & probability scores ---\n",
        "y_pred = softmax_model.predict(X_test_scaled)\n",
        "y_proba = softmax_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 10: Build probability dataframe ---\n",
        "proba_df_low = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in softmax_model.classes_]\n",
        ")\n",
        "proba_df_low[\"Predicted\"] = y_pred\n",
        "proba_df_low[\"Confidence\"] = proba_df_low[[f\"P_{cls}\" for cls in softmax_model.classes_]].max(axis=1)\n",
        "\n",
        "# Add back corresponding Date and Close price for backtesting\n",
        "proba_df_low[\"Date\"] = test_df[\"Date\"].values\n",
        "proba_df_low[\"Close_SOLARINDS.NS\"] = test_df[\"Close_SOLARINDS.NS\"].values\n",
        "\n",
        "# --- Step 11: Save or preview ---\n",
        "proba_df_low.to_csv(\"probability_scores_low.csv\", index=False)\n",
        "print(\"\\nProbability scores (first 10 samples):\")\n",
        "print(proba_df_low.head(10))\n",
        "\n",
        "# --- Step 12: Evaluation ---\n",
        "print(\"\\nModel Evaluation:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))\n"
      ],
      "metadata": {
        "id": "xtIjSSaaREN8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2bc1852-f5f0-4d27-f7c9-15b07262352c"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature importances (coefficients):\n",
            "             Feature  Coefficient\n",
            "8           RSI_Avg3     1.557316\n",
            "5   Lag_Weighted_Avg     0.888688\n",
            "1               Lag2     0.761340\n",
            "3               Lag4     0.485762\n",
            "4               Lag5     0.470908\n",
            "0               Lag1     0.457671\n",
            "7                RSI     0.402753\n",
            "2               Lag3     0.396377\n",
            "10       EMA_Compare     0.276289\n",
            "9                EMA     0.007682\n",
            "6             Volume    -0.076978\n",
            "11      EMA_Diff_Pct    -3.541403\n",
            "\n",
            "Probability scores (first 10 samples):\n",
            "        P_0       P_1       P_2  Predicted  Confidence       Date  \\\n",
            "0  0.033748  0.341400  0.624852          2    0.624852 2020-01-02   \n",
            "1  0.098264  0.390683  0.511053          2    0.511053 2020-01-03   \n",
            "2  0.884583  0.110123  0.005294          0    0.884583 2020-01-06   \n",
            "3  0.686958  0.290754  0.022287          0    0.686958 2020-01-10   \n",
            "4  0.401624  0.492245  0.106131          1    0.492245 2020-01-13   \n",
            "5  0.207945  0.585106  0.206949          1    0.585106 2020-01-14   \n",
            "6  0.024364  0.262991  0.712645          2    0.712645 2020-01-15   \n",
            "7  0.515287  0.419841  0.064872          0    0.515287 2020-01-16   \n",
            "8  0.282634  0.448885  0.268482          1    0.448885 2020-01-17   \n",
            "9  0.338083  0.499293  0.162624          1    0.499293 2020-01-20   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1080.369019  \n",
            "1         1101.244751  \n",
            "2         1096.665771  \n",
            "3         1085.243164  \n",
            "4         1080.122803  \n",
            "5         1079.728882  \n",
            "6         1102.623291  \n",
            "7         1103.361938  \n",
            "8         1117.541748  \n",
            "9         1127.684204  \n",
            "\n",
            "Model Evaluation:\n",
            "[[110   9  26]\n",
            " [109  67 116]\n",
            " [  5  11 130]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.49      0.76      0.60       145\n",
            "        HOLD       0.77      0.23      0.35       292\n",
            "          UP       0.48      0.89      0.62       146\n",
            "\n",
            "    accuracy                           0.53       583\n",
            "   macro avg       0.58      0.63      0.52       583\n",
            "weighted avg       0.63      0.53      0.48       583\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backtesting on softmax regression strategy"
      ],
      "metadata": {
        "id": "ZcLe1yHqsmHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming these three dataframes already exist from your earlier steps:\n",
        "# proba_df_low, proba_df_med, proba_df_high\n",
        "\n",
        "# --- Step 1: Add a column to identify the volatility regime ---\n",
        "proba_df_low[\"Regime\"] = \"Low Volatility\"\n",
        "proba_df_med[\"Regime\"] = \"Medium Volatility\"\n",
        "proba_df_high[\"Regime\"] = \"High Volatility\"\n",
        "\n",
        "# --- Step 2: Combine all three dataframes ---\n",
        "combined_proba_df = pd.concat([proba_df_low, proba_df_med, proba_df_high], ignore_index=True)\n",
        "\n",
        "# --- Step 3: Shuffle the order to simulate mixed trading days ---\n",
        "combined_proba_df = combined_proba_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# --- Step 4: Optional — sort by date if you later need chronological order ---\n",
        "# combined_proba_df = combined_proba_df.sort_values(by=\"Date\").reset_index(drop=True)\n",
        "\n",
        "# --- Step 5: Preview the result ---\n",
        "print(\"\\nCombined probability DataFrame (first 10 rows):\")\n",
        "print(combined_proba_df.head(10))\n",
        "\n",
        "# --- Step 6: Save for backtesting step ---\n",
        "combined_proba_df.to_csv(\"combined_probability_scores.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Goi9SCFslBi",
        "outputId": "967b471e-d937-47b9-ddcf-f6ec88f61522"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Combined probability DataFrame (first 10 rows):\n",
            "        P_0       P_1       P_2  Predicted  Confidence       Date  \\\n",
            "0  0.922431  0.076851  0.000719          0    0.922431 2023-03-09   \n",
            "1  0.311463  0.606967  0.081571          1    0.606967 2020-12-15   \n",
            "2  0.693553  0.272655  0.033792          0    0.693553 2025-02-28   \n",
            "3  0.074343  0.246516  0.679141          2    0.679141 2024-04-25   \n",
            "4  0.077287  0.290879  0.631835          2    0.631835 2020-03-03   \n",
            "5  0.000026  0.062764  0.937210          2    0.937210 2021-10-19   \n",
            "6  0.028651  0.149270  0.822079          2    0.822079 2022-07-20   \n",
            "7  0.997351  0.002641  0.000008          0    0.997351 2025-08-08   \n",
            "8  0.264446  0.392157  0.343397          1    0.392157 2021-09-13   \n",
            "9  0.967033  0.029515  0.003452          0    0.967033 2025-09-18   \n",
            "\n",
            "   Close_SOLARINDS.NS             Regime  \n",
            "0         3771.808838     Low Volatility  \n",
            "1         1076.183594    High Volatility  \n",
            "2         8705.388672     Low Volatility  \n",
            "3         8789.820312     Low Volatility  \n",
            "4         1122.120483    High Volatility  \n",
            "5         2698.015137  Medium Volatility  \n",
            "6         2706.087402  Medium Volatility  \n",
            "7        14462.000000     Low Volatility  \n",
            "8         1794.354248     Low Volatility  \n",
            "9        14617.000000     Low Volatility  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --------------------------------------------\n",
        "# Combine probability dataframes and sort by date\n",
        "# --------------------------------------------\n",
        "proba_combined_df = pd.concat([proba_df_low, proba_df_med, proba_df_high], ignore_index=True)\n",
        "\n",
        "# Sort chronologically\n",
        "proba_combined_df = proba_combined_df.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "# Display summary\n",
        "print(f\"✅ Combined probability dataframe created. Total records: {len(proba_combined_df)}\")\n",
        "print(proba_combined_df.head())\n",
        "\n",
        "proba_combined_df.to_csv(\"combined_probability_scores_sorted.csv\", index=False)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lm314g7vuVNY",
        "outputId": "89107d13-7275-4291-b4e3-ef4c382780a1"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Combined probability dataframe created. Total records: 1428\n",
            "        P_0       P_1       P_2  Predicted  Confidence       Date  \\\n",
            "0  0.033748  0.341400  0.624852          2    0.624852 2020-01-02   \n",
            "1  0.098264  0.390683  0.511053          2    0.511053 2020-01-03   \n",
            "2  0.884583  0.110123  0.005294          0    0.884583 2020-01-06   \n",
            "3  0.840056  0.156285  0.003659          0    0.840056 2020-01-07   \n",
            "4  0.753697  0.234266  0.012037          0    0.753697 2020-01-08   \n",
            "\n",
            "   Close_SOLARINDS.NS             Regime  \n",
            "0         1080.369019     Low Volatility  \n",
            "1         1101.244751     Low Volatility  \n",
            "2         1096.665771     Low Volatility  \n",
            "3         1089.772949  Medium Volatility  \n",
            "4         1087.114258  Medium Volatility  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# SOFTMAX LOGISTIC REGRESSION BACKTEST WITH REALISTIC TRADING COSTS\n",
        "# File: softmax_backtest_realistic.py\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRANSACTION COSTS & SLIPPAGE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class TradingCosts:\n",
        "    \"\"\"\n",
        "    India-specific transaction costs (NSE equity trading)\n",
        "    Based on typical retail brokerage and regulatory fees as of 2024\n",
        "    \"\"\"\n",
        "    # Brokerage fees\n",
        "    BROKERAGE_BUY = 0.0003      # 0.03% on buy side\n",
        "    BROKERAGE_SELL = 0.0003     # 0.03% on sell side\n",
        "\n",
        "    # Securities Transaction Tax (STT)\n",
        "    STT_BUY = 0.0                # No STT on buy\n",
        "    STT_SELL = 0.00025           # 0.025% on sell (equity segment)\n",
        "\n",
        "    # GST on brokerage (18% in India)\n",
        "    GST_RATE = 0.18\n",
        "\n",
        "    # Exchange & clearing charges (combined)\n",
        "    EXCHANGE_CHARGES = 0.000005   # ~0.0005%\n",
        "    CLEARING_CHARGES = 0.000005\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_transaction_cost(price, quantity, side='buy'):\n",
        "        \"\"\"\n",
        "        Calculate all-in transaction cost for a trade\n",
        "\n",
        "        Args:\n",
        "            price (float): Trade execution price\n",
        "            quantity (int): Number of shares\n",
        "            side (str): 'buy' or 'sell'\n",
        "\n",
        "        Returns:\n",
        "            float: Total transaction cost in rupees\n",
        "        \"\"\"\n",
        "        trade_value = price * quantity\n",
        "\n",
        "        # Brokerage\n",
        "        if side == 'buy':\n",
        "            brokerage = trade_value * TradingCosts.BROKERAGE_BUY\n",
        "        else:\n",
        "            brokerage = trade_value * TradingCosts.BROKERAGE_SELL\n",
        "\n",
        "        # GST on brokerage\n",
        "        gst = brokerage * TradingCosts.GST_RATE\n",
        "\n",
        "        # STT (only on sell)\n",
        "        stt = trade_value * TradingCosts.STT_SELL if side == 'sell' else 0\n",
        "\n",
        "        # Exchange & clearing charges\n",
        "        exchange_clearing = trade_value * (TradingCosts.EXCHANGE_CHARGES + TradingCosts.CLEARING_CHARGES)\n",
        "\n",
        "        total_cost = brokerage + gst + stt + exchange_clearing\n",
        "        return total_cost\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_total_round_trip_cost(entry_price, exit_price, quantity):\n",
        "        \"\"\"Calculate total cost for a complete buy-sell round trip\"\"\"\n",
        "        buy_cost = TradingCosts.calculate_transaction_cost(entry_price, quantity, side='buy')\n",
        "        sell_cost = TradingCosts.calculate_transaction_cost(exit_price, quantity, side='sell')\n",
        "        return buy_cost + sell_cost\n",
        "\n",
        "\n",
        "class SlippageModel:\n",
        "    \"\"\"\n",
        "    Dynamic slippage model based on market conditions\n",
        "    Slippage = difference between expected and actual execution price\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, volatility_column='RSI'):\n",
        "        self.volatility_column = volatility_column\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_slippage(price, volatility_level=50, order_size=1):\n",
        "        \"\"\"\n",
        "        Calculate realistic slippage percentage\n",
        "\n",
        "        Args:\n",
        "            price (float): Current price level\n",
        "            volatility_level (float): Market volatility (0-100 scale, e.g., RSI)\n",
        "            order_size (int): Number of shares\n",
        "\n",
        "        Returns:\n",
        "            float: Slippage percentage (as decimal)\n",
        "        \"\"\"\n",
        "        # Base slippage for liquid stocks\n",
        "        base_slippage = 0.0005  # 0.05% baseline\n",
        "\n",
        "        # Volatility-based adjustment\n",
        "        if volatility_level > 70:\n",
        "            vol_adjustment = 0.005   # High volatility: +0.5%\n",
        "        elif volatility_level > 50:\n",
        "            vol_adjustment = 0.003   # Medium volatility: +0.3%\n",
        "        else:\n",
        "            vol_adjustment = 0.0010  # Low volatility: +0.1%\n",
        "\n",
        "        # Order size impact (larger orders face worse execution)\n",
        "        # For 1 share: minimal impact; for 100+ shares: noticeable slippage\n",
        "        if order_size > 50:\n",
        "            size_adjustment = order_size * 0.00001  # +0.001% per share\n",
        "        else:\n",
        "            size_adjustment = 0\n",
        "\n",
        "        total_slippage = base_slippage + vol_adjustment + size_adjustment\n",
        "        return total_slippage\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_slippage_to_price(price, slippage_pct, side='buy'):\n",
        "        \"\"\"\n",
        "        Apply slippage to execution price\n",
        "\n",
        "        Args:\n",
        "            price (float): Ideal execution price\n",
        "            slippage_pct (float): Slippage percentage (as decimal)\n",
        "            side (str): 'buy' (gets worse price) or 'sell' (gets worse price)\n",
        "\n",
        "        Returns:\n",
        "            float: Actual execution price after slippage\n",
        "        \"\"\"\n",
        "        if side == 'buy':\n",
        "            # Buy: you get worse (higher) price\n",
        "            return price * (1 + slippage_pct)\n",
        "        else:\n",
        "            # Sell/Short cover: you get worse (lower) price\n",
        "            return price * (1 - slippage_pct)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN BACKTEST CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class RealisticBacktestingSoftmax:\n",
        "    \"\"\"\n",
        "    Advanced backtesting for Softmax Logistic Regression with:\n",
        "    - Transaction costs (brokerage, STT, GST, exchange charges)\n",
        "    - Realistic slippage model\n",
        "    - Stop-loss / Take-profit logic\n",
        "    - Risk management\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probability_file_path, confidence_threshold=0.60,\n",
        "                 stop_loss_pct=0.02, take_profit_pct=0.05,\n",
        "                 account_capital=100000, risk_per_trade=0.02):\n",
        "        \"\"\"\n",
        "        Initialize realistic backtesting strategy\n",
        "\n",
        "        Args:\n",
        "            probability_file_path (str): Path to probability scores CSV\n",
        "            confidence_threshold (float): Min confidence to trade (0.60 = 60%)\n",
        "            stop_loss_pct (float): Exit if loss exceeds this pct (default 2%)\n",
        "            take_profit_pct (float): Exit if gain reaches this pct (default 5%)\n",
        "            account_capital (float): Total trading capital in rupees\n",
        "            risk_per_trade (float): Risk per trade as % of capital (default 2%)\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(probability_file_path)\n",
        "        self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
        "        self.df = self.df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.stop_loss_pct = stop_loss_pct\n",
        "        self.take_profit_pct = take_profit_pct\n",
        "        self.account_capital = account_capital\n",
        "        self.risk_per_trade = risk_per_trade\n",
        "\n",
        "        # Calculate position size based on risk management\n",
        "        self.position_size = self._calculate_position_size()\n",
        "\n",
        "        # Initialize tracking\n",
        "        self.cumulative_pnl = 0\n",
        "        self.cumulative_costs = 0\n",
        "        self.entry_price = 0\n",
        "        self.position = 0\n",
        "\n",
        "        self.results = {\n",
        "            'Date': [],\n",
        "            'Close': [],\n",
        "            'Confidence': [],\n",
        "            'Predicted': [],\n",
        "            'Action': [],\n",
        "            'Entry_Price': [],\n",
        "            'Exit_Price': [],\n",
        "            'Position_Size': [],\n",
        "            'Transaction_Cost': [],\n",
        "            'Slippage_Cost': [],\n",
        "            'Daily_PNL_Gross': [],\n",
        "            'Daily_PNL_Net': [],\n",
        "            'Cumulative_PNL': [],\n",
        "            'Total_Costs': []\n",
        "        }\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"REALISTIC SOFTMAX BACKTEST CONFIGURATION\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Confidence Threshold: {self.confidence_threshold*100:.0f}%\")\n",
        "        print(f\"Stop Loss: {self.stop_loss_pct*100:.1f}%\")\n",
        "        print(f\"Take Profit: {self.take_profit_pct*100:.1f}%\")\n",
        "        print(f\"Account Capital: ₹{self.account_capital:,.0f}\")\n",
        "        print(f\"Risk Per Trade: {self.risk_per_trade*100:.1f}%\")\n",
        "        print(f\"Position Size: {self.position_size} shares\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    def _calculate_position_size(self):\n",
        "        \"\"\"Calculate position size based on Kelly Criterion principles\"\"\"\n",
        "        # Risk amount per trade\n",
        "        risk_amount = self.account_capital * self.risk_per_trade\n",
        "\n",
        "        # Assume worst-case stop loss distance (2%)\n",
        "        assumed_entry = 1000\n",
        "        stop_loss_distance = assumed_entry * self.stop_loss_pct\n",
        "\n",
        "        # Position size = risk_amount / stop_loss_distance\n",
        "        position_size = max(1, int(risk_amount / stop_loss_distance))\n",
        "        return position_size\n",
        "\n",
        "    def execute_backtest(self):\n",
        "        \"\"\"Execute realistic backtest with all costs and risk management\"\"\"\n",
        "\n",
        "        # Day 0: Check for initial signal\n",
        "        day0_close = self.df.loc[0, 'Close_SOLARINDS.NS']\n",
        "        day0_confidence = self.df.loc[0, 'Confidence']\n",
        "        day0_signal = self.df.loc[0, 'Predicted']\n",
        "\n",
        "        if day0_confidence >= self.confidence_threshold and day0_signal in [0, 2]:\n",
        "            # Get slippage\n",
        "            rsi = self.df.loc[0, 'RSI'] if 'RSI' in self.df.columns else 50\n",
        "            slippage_pct = SlippageModel.calculate_slippage(day0_close, rsi, self.position_size)\n",
        "\n",
        "            if day0_signal == 2:  # UP - BUY\n",
        "                # Entry: Buy with slippage\n",
        "                entry_price_ideal = day0_close\n",
        "                entry_price_actual = SlippageModel.apply_slippage_to_price(\n",
        "                    entry_price_ideal, slippage_pct, side='buy'\n",
        "                )\n",
        "\n",
        "                # Transaction cost on buy\n",
        "                txn_cost = TradingCosts.calculate_transaction_cost(\n",
        "                    entry_price_actual, self.position_size, side='buy'\n",
        "                )\n",
        "\n",
        "                # Exit: Settle at next day close\n",
        "                if len(self.df) > 1:\n",
        "                    exit_price_ideal = self.df.loc[1, 'Close_SOLARINDS.NS']\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(\n",
        "                        exit_price_ideal,\n",
        "                        self.df.loc[1, 'RSI'] if 'RSI' in self.df.columns else 50,\n",
        "                        self.position_size\n",
        "                    )\n",
        "                    exit_price_actual = SlippageModel.apply_slippage_to_price(\n",
        "                        exit_price_ideal, exit_slippage_pct, side='sell'\n",
        "                    )\n",
        "                else:\n",
        "                    exit_price_actual = entry_price_actual\n",
        "\n",
        "                # Transaction cost on sell\n",
        "                sell_txn_cost = TradingCosts.calculate_transaction_cost(\n",
        "                    exit_price_actual, self.position_size, side='sell'\n",
        "                )\n",
        "\n",
        "                total_txn_cost = txn_cost + sell_txn_cost\n",
        "\n",
        "                # PNL calculation\n",
        "                gross_pnl = (exit_price_actual - entry_price_actual) * self.position_size\n",
        "                net_pnl = gross_pnl - total_txn_cost\n",
        "\n",
        "                self.cumulative_pnl += net_pnl\n",
        "                self.cumulative_costs += total_txn_cost\n",
        "\n",
        "                slippage_cost = (slippage_pct + exit_slippage_pct) * (entry_price_actual * self.position_size)\n",
        "\n",
        "                action = 'BUY'\n",
        "\n",
        "                print(f\"Day 0: BUY | Entry: ₹{entry_price_actual:.2f}, Exit: ₹{exit_price_actual:.2f}\")\n",
        "                print(f\"  Gross PNL: ₹{gross_pnl:.2f}, Net PNL: ₹{net_pnl:.2f}, Costs: ₹{total_txn_cost:.2f}\")\n",
        "\n",
        "            else:  # DOWN - SHORT\n",
        "                entry_price_ideal = day0_close\n",
        "                entry_price_actual = SlippageModel.apply_slippage_to_price(\n",
        "                    entry_price_ideal, slippage_pct, side='sell'\n",
        "                )\n",
        "\n",
        "                txn_cost = TradingCosts.calculate_transaction_cost(\n",
        "                    entry_price_actual, self.position_size, side='sell'\n",
        "                )\n",
        "\n",
        "                if len(self.df) > 1:\n",
        "                    exit_price_ideal = self.df.loc[1, 'Close_SOLARINDS.NS']\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(\n",
        "                        exit_price_ideal,\n",
        "                        self.df.loc[1, 'RSI'] if 'RSI' in self.df.columns else 50,\n",
        "                        self.position_size\n",
        "                    )\n",
        "                    exit_price_actual = SlippageModel.apply_slippage_to_price(\n",
        "                        exit_price_ideal, exit_slippage_pct, side='buy'\n",
        "                    )\n",
        "                else:\n",
        "                    exit_price_actual = entry_price_actual\n",
        "\n",
        "                buy_txn_cost = TradingCosts.calculate_transaction_cost(\n",
        "                    exit_price_actual, self.position_size, side='buy'\n",
        "                )\n",
        "\n",
        "                total_txn_cost = txn_cost + buy_txn_cost\n",
        "\n",
        "                # SHORT: profit = entry - exit\n",
        "                gross_pnl = (entry_price_actual - exit_price_actual) * self.position_size\n",
        "                net_pnl = gross_pnl - total_txn_cost\n",
        "\n",
        "                self.cumulative_pnl += net_pnl\n",
        "                self.cumulative_costs += total_txn_cost\n",
        "\n",
        "                slippage_cost = (slippage_pct + exit_slippage_pct) * (entry_price_actual * self.position_size)\n",
        "\n",
        "                action = 'SHORT'\n",
        "\n",
        "                print(f\"Day 0: SHORT | Entry: ₹{entry_price_actual:.2f}, Exit: ₹{exit_price_actual:.2f}\")\n",
        "                print(f\"  Gross PNL: ₹{gross_pnl:.2f}, Net PNL: ₹{net_pnl:.2f}, Costs: ₹{total_txn_cost:.2f}\")\n",
        "\n",
        "        else:\n",
        "            action = 'NO_TRADE'\n",
        "            entry_price_actual = None\n",
        "            exit_price_actual = None\n",
        "            gross_pnl = 0\n",
        "            net_pnl = 0\n",
        "            total_txn_cost = 0\n",
        "            slippage_cost = 0\n",
        "\n",
        "        # Record day 0\n",
        "        self.results['Date'].append(self.df.loc[0, 'Date'])\n",
        "        self.results['Close'].append(day0_close)\n",
        "        self.results['Confidence'].append(day0_confidence)\n",
        "        self.results['Predicted'].append(day0_signal)\n",
        "        self.results['Action'].append(action)\n",
        "        self.results['Entry_Price'].append(entry_price_actual)\n",
        "        self.results['Exit_Price'].append(exit_price_actual)\n",
        "        self.results['Position_Size'].append(self.position_size if action != 'NO_TRADE' else 0)\n",
        "        self.results['Transaction_Cost'].append(total_txn_cost)\n",
        "        self.results['Slippage_Cost'].append(slippage_cost)\n",
        "        self.results['Daily_PNL_Gross'].append(gross_pnl)\n",
        "        self.results['Daily_PNL_Net'].append(net_pnl)\n",
        "        self.results['Cumulative_PNL'].append(self.cumulative_pnl)\n",
        "        self.results['Total_Costs'].append(self.cumulative_costs)\n",
        "\n",
        "        # Days 1 onwards\n",
        "        for i in range(1, len(self.df)):\n",
        "            current_date = self.df.loc[i, 'Date']\n",
        "            current_close = self.df.loc[i, 'Close_SOLARINDS.NS']\n",
        "            current_confidence = self.df.loc[i, 'Confidence']\n",
        "            signal_class = self.df.loc[i, 'Predicted']\n",
        "            rsi = self.df.loc[i, 'RSI'] if 'RSI' in self.df.columns else 50\n",
        "\n",
        "            # Skip low confidence trades\n",
        "            if current_confidence < self.confidence_threshold or signal_class == 1:  # HOLD\n",
        "                action = 'NO_TRADE'\n",
        "                gross_pnl = 0\n",
        "                net_pnl = 0\n",
        "                total_txn_cost = 0\n",
        "                slippage_cost = 0\n",
        "                entry_price_actual = None\n",
        "                exit_price_actual = None\n",
        "            else:\n",
        "                prev_close = self.df.loc[i-1, 'Close_SOLARINDS.NS']\n",
        "\n",
        "                if signal_class == 2:  # BUY\n",
        "                    slippage_pct = SlippageModel.calculate_slippage(prev_close, rsi, self.position_size)\n",
        "                    entry_actual = SlippageModel.apply_slippage_to_price(prev_close, slippage_pct, side='buy')\n",
        "                    entry_price_actual = entry_actual\n",
        "\n",
        "                    buy_txn = TradingCosts.calculate_transaction_cost(entry_actual, self.position_size, side='buy')\n",
        "\n",
        "                    # Exit at current close with slippage\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(current_close, rsi, self.position_size)\n",
        "                    exit_actual = SlippageModel.apply_slippage_to_price(current_close, exit_slippage_pct, side='sell')\n",
        "                    exit_price_actual = exit_actual\n",
        "\n",
        "                    sell_txn = TradingCosts.calculate_transaction_cost(exit_actual, self.position_size, side='sell')\n",
        "\n",
        "                    total_txn_cost = buy_txn + sell_txn\n",
        "                    slippage_cost = (slippage_pct + exit_slippage_pct) * (entry_actual * self.position_size)\n",
        "\n",
        "                    # Gross PNL\n",
        "                    gross_pnl = (exit_actual - entry_actual) * self.position_size\n",
        "\n",
        "                    # Check stop loss\n",
        "                    loss_pct = (entry_actual - exit_actual) / entry_actual\n",
        "                    if loss_pct > self.stop_loss_pct:\n",
        "                        action = 'BUY_STOP_LOSS'\n",
        "                        net_pnl = -self.account_capital * self.risk_per_trade - total_txn_cost\n",
        "                    else:\n",
        "                        action = 'BUY'\n",
        "                        net_pnl = gross_pnl - total_txn_cost\n",
        "\n",
        "                else:  # SHORT\n",
        "                    slippage_pct = SlippageModel.calculate_slippage(prev_close, rsi, self.position_size)\n",
        "                    entry_actual = SlippageModel.apply_slippage_to_price(prev_close, slippage_pct, side='sell')\n",
        "                    entry_price_actual = entry_actual\n",
        "\n",
        "                    sell_txn = TradingCosts.calculate_transaction_cost(entry_actual, self.position_size, side='sell')\n",
        "\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(current_close, rsi, self.position_size)\n",
        "                    exit_actual = SlippageModel.apply_slippage_to_price(current_close, exit_slippage_pct, side='buy')\n",
        "                    exit_price_actual = exit_actual\n",
        "\n",
        "                    buy_txn = TradingCosts.calculate_transaction_cost(exit_actual, self.position_size, side='buy')\n",
        "\n",
        "                    total_txn_cost = sell_txn + buy_txn\n",
        "                    slippage_cost = (slippage_pct + exit_slippage_pct) * (entry_actual * self.position_size)\n",
        "\n",
        "                    # SHORT: profit = entry - exit\n",
        "                    gross_pnl = (entry_actual - exit_actual) * self.position_size\n",
        "\n",
        "                    # Check stop loss\n",
        "                    loss_pct = (exit_actual - entry_actual) / entry_actual\n",
        "                    if loss_pct > self.stop_loss_pct:\n",
        "                        action = 'SHORT_STOP_LOSS'\n",
        "                        net_pnl = -self.account_capital * self.risk_per_trade - total_txn_cost\n",
        "                    else:\n",
        "                        action = 'SHORT'\n",
        "                        net_pnl = gross_pnl - total_txn_cost\n",
        "\n",
        "                self.cumulative_pnl += net_pnl\n",
        "                self.cumulative_costs += total_txn_cost\n",
        "\n",
        "            # Record trade\n",
        "            self.results['Date'].append(current_date)\n",
        "            self.results['Close'].append(current_close)\n",
        "            self.results['Confidence'].append(current_confidence)\n",
        "            self.results['Predicted'].append(signal_class)\n",
        "            self.results['Action'].append(action)\n",
        "            self.results['Entry_Price'].append(entry_price_actual)\n",
        "            self.results['Exit_Price'].append(exit_price_actual)\n",
        "            self.results['Position_Size'].append(self.position_size if action != 'NO_TRADE' else 0)\n",
        "            self.results['Transaction_Cost'].append(total_txn_cost)\n",
        "            self.results['Slippage_Cost'].append(slippage_cost)\n",
        "            self.results['Daily_PNL_Gross'].append(gross_pnl)\n",
        "            self.results['Daily_PNL_Net'].append(net_pnl)\n",
        "            self.results['Cumulative_PNL'].append(self.cumulative_pnl)\n",
        "            self.results['Total_Costs'].append(self.cumulative_costs)\n",
        "\n",
        "        return pd.DataFrame(self.results)\n",
        "\n",
        "    def calculate_metrics(self, results_df):\n",
        "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "        initial_price = self.df.loc[0, 'Close_SOLARINDS.NS']\n",
        "        final_price = self.df.iloc[-1]['Close_SOLARINDS.NS']\n",
        "        final_pnl = results_df['Cumulative_PNL'].iloc[-1]\n",
        "        total_costs = results_df['Total_Costs'].iloc[-1]\n",
        "\n",
        "        buy_hold_return = ((final_price / initial_price) - 1) * 100\n",
        "        strategy_return = (final_pnl / self.account_capital) * 100\n",
        "\n",
        "        # Drawdown\n",
        "        cumulative_max = results_df['Cumulative_PNL'].cummax()\n",
        "        drawdown = (results_df['Cumulative_PNL'] - cumulative_max) / (cumulative_max + 1e-10)\n",
        "        max_drawdown = drawdown.min() * 100\n",
        "\n",
        "        # Sharpe ratio\n",
        "        daily_pnl = results_df['Daily_PNL_Net']\n",
        "        sharpe = (daily_pnl.mean() / daily_pnl.std()) * np.sqrt(252) if daily_pnl.std() != 0 else 0\n",
        "\n",
        "        # Win rate\n",
        "        profitable = len(results_df[results_df['Daily_PNL_Net'] > 0])\n",
        "        total_trades = len(results_df[results_df['Daily_PNL_Net'] != 0])\n",
        "        win_rate = (profitable / total_trades) * 100 if total_trades > 0 else 0\n",
        "\n",
        "        metrics = {\n",
        "            'Initial_Price': initial_price,\n",
        "            'Final_Price': final_price,\n",
        "            'Buy_Hold_Return': buy_hold_return,\n",
        "            'Strategy_Return': strategy_return,\n",
        "            'Total_PNL': final_pnl,\n",
        "            'Total_Costs': total_costs,\n",
        "            'Max_Drawdown': max_drawdown,\n",
        "            'Sharpe_Ratio': sharpe,\n",
        "            'Win_Rate': win_rate,\n",
        "            'Trading_Days': total_trades,\n",
        "            'Total_Days': len(results_df)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def print_summary(self, metrics):\n",
        "        \"\"\"Print comprehensive performance summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"REALISTIC SOFTMAX BACKTEST RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nPrice Movement:\")\n",
        "        print(f\"  Initial: ₹{metrics['Initial_Price']:.2f}\")\n",
        "        print(f\"  Final: ₹{metrics['Final_Price']:.2f}\")\n",
        "        print(f\"  Buy & Hold Return: {metrics['Buy_Hold_Return']:.2f}%\")\n",
        "        print(f\"\\nStrategy Performance:\")\n",
        "        print(f\"  Total PNL: ₹{metrics['Total_PNL']:.2f}\")\n",
        "        print(f\"  Strategy Return: {metrics['Strategy_Return']:.2f}%\")\n",
        "        print(f\"  Total Costs (txn + slippage): ₹{metrics['Total_Costs']:.2f}\")\n",
        "        print(f\"\\nRisk Metrics:\")\n",
        "        print(f\"  Max Drawdown: {metrics['Max_Drawdown']:.2f}%\")\n",
        "        print(f\"  Sharpe Ratio: {metrics['Sharpe_Ratio']:.2f}\")\n",
        "        print(f\"  Win Rate: {metrics['Win_Rate']:.2f}%\")\n",
        "        print(f\"\\nTrading Activity:\")\n",
        "        print(f\"  Trading Days: {metrics['Trading_Days']} / {metrics['Total_Days']}\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    def export_results(self, results_df, csv_path='softmax_backtest_realistic.csv'):\n",
        "        \"\"\"Export detailed results\"\"\"\n",
        "        results_df.to_csv(csv_path, index=False)\n",
        "        print(f\"Results exported to '{csv_path}'\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    backtest = RealisticBacktestingSoftmax(\n",
        "        probability_file_path='combined_probability_scores_sorted.csv',\n",
        "        confidence_threshold=0.60,\n",
        "        stop_loss_pct=0.02,\n",
        "        take_profit_pct=0.05,\n",
        "        account_capital=100000,\n",
        "        risk_per_trade=0.02\n",
        "    )\n",
        "\n",
        "    print(\"\\nExecuting realistic backtest...\\n\")\n",
        "    results_df = backtest.execute_backtest()\n",
        "\n",
        "    metrics = backtest.calculate_metrics(results_df)\n",
        "    backtest.print_summary(metrics)\n",
        "\n",
        "    backtest.export_results(results_df)\n",
        "\n",
        "    print(\"\\nAction Distribution:\")\n",
        "    print(results_df['Action'].value_counts())\n",
        "\n",
        "    print(\"\\nTrading Days Summary (first 20):\")\n",
        "    trading_days = results_df[results_df['Daily_PNL_Net'] != 0]\n",
        "    print(trading_days[['Date', 'Action', 'Entry_Price', 'Exit_Price',\n",
        "                         'Daily_PNL_Gross', 'Transaction_Cost', 'Daily_PNL_Net']].head(20))\n",
        "\n",
        "    print(\"\\nCost Analysis:\")\n",
        "    print(f\"Total Transaction Costs: ₹{results_df['Transaction_Cost'].sum():.2f}\")\n",
        "    print(f\"Total Slippage Costs: ₹{results_df['Slippage_Cost'].sum():.2f}\")\n",
        "    print(f\"Combined Cost Impact: {(results_df['Transaction_Cost'].sum() + results_df['Slippage_Cost'].sum()) / abs(results_df['Daily_PNL_Gross'].sum()) * 100:.2f}% of gross PNL\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNUQTPgkxz1q",
        "outputId": "40eb1ac8-9bfd-4b02-d612-4aafc9172dac"
      },
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "REALISTIC SOFTMAX BACKTEST CONFIGURATION\n",
            "======================================================================\n",
            "Confidence Threshold: 60%\n",
            "Stop Loss: 2.0%\n",
            "Take Profit: 5.0%\n",
            "Account Capital: ₹100,000\n",
            "Risk Per Trade: 2.0%\n",
            "Position Size: 100 shares\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Executing realistic backtest...\n",
            "\n",
            "Day 0: BUY | Entry: ₹1083.07, Exit: ₹1098.49\n",
            "  Gross PNL: ₹1542.17, Net PNL: ₹1435.30, Costs: ₹106.87\n",
            "\n",
            "======================================================================\n",
            "REALISTIC SOFTMAX BACKTEST RESULTS\n",
            "======================================================================\n",
            "\n",
            "Price Movement:\n",
            "  Initial: ₹1080.37\n",
            "  Final: ₹13901.00\n",
            "  Buy & Hold Return: 1186.69%\n",
            "\n",
            "Strategy Performance:\n",
            "  Total PNL: ₹5360830.35\n",
            "  Strategy Return: 5360.83%\n",
            "  Total Costs (txn + slippage): ₹546116.91\n",
            "\n",
            "Risk Metrics:\n",
            "  Max Drawdown: -38.19%\n",
            "  Sharpe Ratio: 5.32\n",
            "  Win Rate: 69.64%\n",
            "\n",
            "Trading Activity:\n",
            "  Trading Days: 873 / 1428\n",
            "======================================================================\n",
            "\n",
            "Results exported to 'softmax_backtest_realistic.csv'\n",
            "\n",
            "Action Distribution:\n",
            "Action\n",
            "NO_TRADE           555\n",
            "BUY                455\n",
            "SHORT              385\n",
            "SHORT_STOP_LOSS     18\n",
            "BUY_STOP_LOSS       15\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Trading Days Summary (first 20):\n",
            "         Date Action  Entry_Price   Exit_Price  Daily_PNL_Gross  \\\n",
            "0  2020-01-02    BUY  1083.069941  1098.491639      1542.169800   \n",
            "2  2020-01-06  SHORT  1098.491639  1099.407436       -91.579681   \n",
            "3  2020-01-07  SHORT  1093.924107  1092.497382       142.672546   \n",
            "4  2020-01-08  SHORT  1087.048517  1089.832043      -278.352661   \n",
            "6  2020-01-10  SHORT  1091.812334  1087.956272       385.606232   \n",
            "9  2020-01-15    BUY  1082.428204  1099.866733      1743.852875   \n",
            "15 2020-01-23  SHORT  1141.022862  1136.228874       479.398773   \n",
            "16 2020-01-24    BUY  1136.228874  1158.997831      2276.895721   \n",
            "19 2020-01-29    BUY  1207.699708  1250.248138      4254.842957   \n",
            "21 2020-01-31  SHORT  1271.464617  1250.937538      2052.707886   \n",
            "25 2020-02-06    BUY  1249.654309  1255.748878       609.456909   \n",
            "26 2020-02-07    BUY  1262.043358  1257.811458      -423.190094   \n",
            "28 2020-02-11  SHORT  1248.872922  1221.470744      2740.217804   \n",
            "29 2020-02-12  SHORT  1215.378621  1195.853761      1952.485992   \n",
            "30 2020-02-13  SHORT  1189.889403  1170.631561      1925.784210   \n",
            "33 2020-02-18    BUY  1150.049228  1154.282964       423.373596   \n",
            "34 2020-02-19    BUY  1160.068843  1226.035749      6596.690613   \n",
            "36 2020-02-24  SHORT  1205.163135  1189.634516      1552.861847   \n",
            "37 2020-02-25  SHORT  1183.701177  1171.223858      1247.731873   \n",
            "39 2020-02-27  SHORT  1183.406627  1164.807344      1859.928284   \n",
            "\n",
            "    Transaction_Cost  Daily_PNL_Net  \n",
            "0         106.871132    1435.298667  \n",
            "2         107.465817    -199.045499  \n",
            "3         106.933845      35.738702  \n",
            "4         106.414665    -384.767326  \n",
            "6         106.638886     278.967346  \n",
            "9         106.932204    1636.920671  \n",
            "15        111.417535     367.981238  \n",
            "16        112.521198    2164.374524  \n",
            "19        120.725505    4134.117452  \n",
            "21        123.602054    1929.105832  \n",
            "25        122.590398     486.866511  \n",
            "26        123.168002    -546.358096  \n",
            "28        121.142333    2619.075471  \n",
            "29        118.153324    1834.332668  \n",
            "30        115.670198    1810.114012  \n",
            "33        112.734766     310.638830  \n",
            "34        117.505101    6479.185512  \n",
            "36        117.299713    1435.562134  \n",
            "37        115.311801    1132.420072  \n",
            "39        115.060154    1744.868129  \n",
            "\n",
            "Cost Analysis:\n",
            "Total Transaction Costs: ₹546116.91\n",
            "Total Slippage Costs: ₹2785360.13\n",
            "Combined Cost Impact: 64.30% of gross PNL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XGBoost Model"
      ],
      "metadata": {
        "id": "RdGgI0tAM2Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# --- Step 1: Copy dataframe ---\n",
        "df = final_df.copy()\n",
        "\n",
        "# --- Step 2: Encode categorical variables ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Volatility_Regime', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 3: Split train/test by date ---\n",
        "# Ensure 'Date' is datetime\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "train_df = df[df['Date'].dt.year < 2020].copy()   # Train: 2010–2019\n",
        "test_df  = df[df['Date'].dt.year >= 2020].copy()  # Test: 2020–2025\n",
        "\n",
        "print(f\"Training samples: {len(train_df)} | Testing samples: {len(test_df)}\")\n",
        "\n",
        "# --- Step 4: Feature-target split ---\n",
        "feature_cols = [col for col in df.columns if col not in ['Direction', 'Volatility_Regime', 'Close_SOLARINDS.NS', 'Date', 'Year']]\n",
        "X_train = train_df[feature_cols]\n",
        "X_test  = test_df[feature_cols]\n",
        "y_train = train_df['Direction']\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# Keep Date and Close for later (backtest alignment)\n",
        "dates_test = test_df['Date'].reset_index(drop=True)\n",
        "close_test = test_df['Close_SOLARINDS.NS'].reset_index(drop=True)\n",
        "\n",
        "# --- Step 5: Feature scaling ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# --- Step 6: Define and train XGBoost model ---\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(label_encoders['Direction'].classes_),\n",
        "    eval_metric='mlogloss',\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    n_estimators=700,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=17\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 7: Predictions ---\n",
        "y_pred = xgb_model.predict(X_test_scaled)\n",
        "y_proba = xgb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 8: Construct probability dataframe for backtesting ---\n",
        "proba_df = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in label_encoders['Direction'].classes_]\n",
        ")\n",
        "proba_df[\"Predicted\"] = label_encoders['Direction'].inverse_transform(y_pred)\n",
        "proba_df[\"True\"] = label_encoders['Direction'].inverse_transform(y_test.values)\n",
        "proba_df[\"Confidence\"] = proba_df[[f\"P_{cls}\" for cls in label_encoders['Direction'].classes_]].max(axis=1)\n",
        "proba_df[\"Date\"] = dates_test\n",
        "proba_df[\"Close_SOLARINDS.NS\"] = close_test\n",
        "\n",
        "# --- Step 9: Sort by date ---\n",
        "proba_df = proba_df.sort_values(by=\"Date\").reset_index(drop=True)\n",
        "\n",
        "# --- Step 10: Save and preview ---\n",
        "proba_df.to_csv(\"XGB_Probability_Scores_Sorted.csv\", index=False)\n",
        "\n",
        "print(\"✅ Probability DataFrame created and sorted by date.\")\n",
        "print(proba_df.head())\n",
        "\n",
        "# --- Step 11: Evaluation ---\n",
        "print(\"\\nModel Evaluation (XGBoost):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3sHpEbgU2Xt",
        "outputId": "ba0c871c-7c52-42b1-f5bc-d0c694b133a4"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 2442 | Testing samples: 1428\n",
            "✅ Probability DataFrame created and sorted by date.\n",
            "     P_DOWN    P_HOLD      P_UP Predicted  True  Confidence       Date  \\\n",
            "0  0.000060  0.477030  0.522911        UP    UP    0.522911 2020-01-02   \n",
            "1  0.011550  0.319087  0.669363        UP    UP    0.669363 2020-01-03   \n",
            "2  0.916448  0.069361  0.014191      DOWN  HOLD    0.916448 2020-01-06   \n",
            "3  0.790312  0.209060  0.000628      DOWN  HOLD    0.790312 2020-01-07   \n",
            "4  0.150311  0.847147  0.002542      HOLD  HOLD    0.847147 2020-01-08   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1080.369019  \n",
            "1         1101.244751  \n",
            "2         1096.665771  \n",
            "3         1089.772949  \n",
            "4         1087.114258  \n",
            "\n",
            "Model Evaluation (XGBoost):\n",
            "[[178 143  34]\n",
            " [ 78 463 144]\n",
            " [ 11 125 252]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.67      0.50      0.57       355\n",
            "        HOLD       0.63      0.68      0.65       685\n",
            "          UP       0.59      0.65      0.62       388\n",
            "\n",
            "    accuracy                           0.63      1428\n",
            "   macro avg       0.63      0.61      0.61      1428\n",
            "weighted avg       0.63      0.63      0.62      1428\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# XGBOOST BACKTEST WITH REALISTIC TRADING COSTS\n",
        "# File: xgboost_backtest_realistic.py\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# TRANSACTION COSTS & SLIPPAGE CONFIGURATION\n",
        "# ============================================================================\n",
        "\n",
        "class TradingCosts:\n",
        "    \"\"\"\n",
        "    India-specific transaction costs (NSE equity trading)\n",
        "    Based on typical retail brokerage and regulatory fees as of 2024\n",
        "    \"\"\"\n",
        "    # Brokerage fees\n",
        "    BROKERAGE_BUY = 0.0003      # 0.03% on buy side\n",
        "    BROKERAGE_SELL = 0.0003     # 0.03% on sell side\n",
        "\n",
        "    # Securities Transaction Tax (STT)\n",
        "    STT_BUY = 0.0                # No STT on buy\n",
        "    STT_SELL = 0.00025           # 0.025% on sell (equity segment)\n",
        "\n",
        "    # GST on brokerage (18% in India)\n",
        "    GST_RATE = 0.18\n",
        "\n",
        "    # Exchange & clearing charges (combined)\n",
        "    EXCHANGE_CHARGES = 0.000005   # ~0.0005%\n",
        "    CLEARING_CHARGES = 0.000005\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_transaction_cost(price, quantity, side='buy'):\n",
        "        \"\"\"\n",
        "        Calculate all-in transaction cost for a trade\n",
        "\n",
        "        Args:\n",
        "            price (float): Trade execution price\n",
        "            quantity (int): Number of shares\n",
        "            side (str): 'buy' or 'sell'\n",
        "\n",
        "        Returns:\n",
        "            float: Total transaction cost in rupees\n",
        "        \"\"\"\n",
        "        trade_value = price * quantity\n",
        "\n",
        "        # Brokerage\n",
        "        if side == 'buy':\n",
        "            brokerage = trade_value * TradingCosts.BROKERAGE_BUY\n",
        "        else:\n",
        "            brokerage = trade_value * TradingCosts.BROKERAGE_SELL\n",
        "\n",
        "        # GST on brokerage\n",
        "        gst = brokerage * TradingCosts.GST_RATE\n",
        "\n",
        "        # STT (only on sell)\n",
        "        stt = trade_value * TradingCosts.STT_SELL if side == 'sell' else 0\n",
        "\n",
        "        # Exchange & clearing charges\n",
        "        exchange_clearing = trade_value * (TradingCosts.EXCHANGE_CHARGES + TradingCosts.CLEARING_CHARGES)\n",
        "\n",
        "        total_cost = brokerage + gst + stt + exchange_clearing\n",
        "        return total_cost\n",
        "\n",
        "\n",
        "class SlippageModel:\n",
        "    \"\"\"\n",
        "    Dynamic slippage model based on market conditions\n",
        "    Slippage = difference between expected and actual execution price\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_slippage(price, volatility_level=50, order_size=1):\n",
        "        \"\"\"\n",
        "        Calculate realistic slippage percentage\n",
        "\n",
        "        Args:\n",
        "            price (float): Current price level\n",
        "            volatility_level (float): Market volatility (0-100 scale)\n",
        "            order_size (int): Number of shares\n",
        "\n",
        "        Returns:\n",
        "            float: Slippage percentage (as decimal)\n",
        "        \"\"\"\n",
        "        # Base slippage for liquid stocks\n",
        "        base_slippage = 0.0005  # 0.05% baseline\n",
        "\n",
        "        # Volatility-based adjustment\n",
        "        if volatility_level > 70:\n",
        "            vol_adjustment = 0.005   # High volatility: +0.5%\n",
        "        elif volatility_level > 50:\n",
        "            vol_adjustment = 0.003   # Medium volatility: +0.3%\n",
        "        else:\n",
        "            vol_adjustment = 0.0010  # Low volatility: +0.1%\n",
        "\n",
        "        # Order size impact\n",
        "        if order_size > 50:\n",
        "            size_adjustment = order_size * 0.00001\n",
        "        else:\n",
        "            size_adjustment = 0\n",
        "\n",
        "        total_slippage = base_slippage + vol_adjustment + size_adjustment\n",
        "        return total_slippage\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_slippage_to_price(price, slippage_pct, side='buy'):\n",
        "        \"\"\"\n",
        "        Apply slippage to execution price\n",
        "\n",
        "        Args:\n",
        "            price (float): Ideal execution price\n",
        "            slippage_pct (float): Slippage percentage (as decimal)\n",
        "            side (str): 'buy' (gets worse price) or 'sell' (gets worse price)\n",
        "\n",
        "        Returns:\n",
        "            float: Actual execution price after slippage\n",
        "        \"\"\"\n",
        "        if side == 'buy':\n",
        "            # Buy: you get worse (higher) price\n",
        "            return price * (1 + slippage_pct)\n",
        "        else:\n",
        "            # Sell/Short cover: you get worse (lower) price\n",
        "            return price * (1 - slippage_pct)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN BACKTEST CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class RealisticBacktestingXGBoost:\n",
        "    \"\"\"\n",
        "    Advanced backtesting for XGBoost model with:\n",
        "    - Transaction costs (brokerage, STT, GST, exchange charges)\n",
        "    - Realistic slippage model\n",
        "    - Stop-loss logic\n",
        "    - Risk management\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, probability_file_path, confidence_threshold=0.60,\n",
        "                 stop_loss_pct=0.02, take_profit_pct=0.05,\n",
        "                 account_capital=100000, risk_per_trade=0.02):\n",
        "        \"\"\"\n",
        "        Initialize realistic backtesting strategy\n",
        "\n",
        "        Args:\n",
        "            probability_file_path (str): Path to probability scores CSV\n",
        "            confidence_threshold (float): Min confidence to trade\n",
        "            stop_loss_pct (float): Exit if loss exceeds this pct\n",
        "            take_profit_pct (float): Exit if gain reaches this pct\n",
        "            account_capital (float): Total trading capital in rupees\n",
        "            risk_per_trade (float): Risk per trade as % of capital\n",
        "        \"\"\"\n",
        "        self.df = pd.read_csv(probability_file_path)\n",
        "        self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
        "        self.df = self.df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.stop_loss_pct = stop_loss_pct\n",
        "        self.take_profit_pct = take_profit_pct\n",
        "        self.account_capital = account_capital\n",
        "        self.risk_per_trade = risk_per_trade\n",
        "\n",
        "        # Calculate position size\n",
        "        self.position_size = self._calculate_position_size()\n",
        "\n",
        "        # Initialize tracking\n",
        "        self.cumulative_pnl = 0\n",
        "        self.cumulative_costs = 0\n",
        "\n",
        "        self.results = {\n",
        "            'Date': [],\n",
        "            'Close': [],\n",
        "            'Confidence': [],\n",
        "            'Predicted': [],\n",
        "            'Action': [],\n",
        "            'Entry_Price': [],\n",
        "            'Exit_Price': [],\n",
        "            'Position_Size': [],\n",
        "            'Transaction_Cost': [],\n",
        "            'Slippage_Cost': [],\n",
        "            'Daily_PNL_Gross': [],\n",
        "            'Daily_PNL_Net': [],\n",
        "            'Cumulative_PNL': [],\n",
        "            'Total_Costs': []\n",
        "        }\n",
        "\n",
        "        print(\"=\"*70)\n",
        "        print(\"REALISTIC XGBOOST BACKTEST CONFIGURATION\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"Confidence Threshold: {self.confidence_threshold*100:.0f}%\")\n",
        "        print(f\"Stop Loss: {self.stop_loss_pct*100:.1f}%\")\n",
        "        print(f\"Take Profit: {self.take_profit_pct*100:.1f}%\")\n",
        "        print(f\"Account Capital: ₹{self.account_capital:,.0f}\")\n",
        "        print(f\"Risk Per Trade: {self.risk_per_trade*100:.1f}%\")\n",
        "        print(f\"Position Size: {self.position_size} shares\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    def _calculate_position_size(self):\n",
        "        \"\"\"Calculate position size based on risk management\"\"\"\n",
        "        risk_amount = self.account_capital * self.risk_per_trade\n",
        "        assumed_entry = 1000\n",
        "        stop_loss_distance = assumed_entry * self.stop_loss_pct\n",
        "        position_size = max(1, int(risk_amount / stop_loss_distance))\n",
        "        return position_size\n",
        "\n",
        "    def execute_backtest(self):\n",
        "        \"\"\"Execute realistic backtest with all costs and risk management\"\"\"\n",
        "\n",
        "        # Day 0\n",
        "        day0_close = self.df.loc[0, 'Close_SOLARINDS.NS']\n",
        "        day0_confidence = self.df.loc[0, 'Confidence']\n",
        "        day0_signal = self.df.loc[0, 'Predicted']  # 'UP', 'DOWN', 'HOLD'\n",
        "\n",
        "        if day0_confidence >= self.confidence_threshold and day0_signal != 'HOLD':\n",
        "            slippage_pct = SlippageModel.calculate_slippage(day0_close, 50, self.position_size)\n",
        "\n",
        "            if day0_signal == 'UP':  # BUY\n",
        "                entry_ideal = day0_close\n",
        "                entry_actual = SlippageModel.apply_slippage_to_price(entry_ideal, slippage_pct, side='buy')\n",
        "                txn_cost = TradingCosts.calculate_transaction_cost(entry_actual, self.position_size, side='buy')\n",
        "\n",
        "                if len(self.df) > 1:\n",
        "                    exit_ideal = self.df.loc[1, 'Close_SOLARINDS.NS']\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(exit_ideal, 50, self.position_size)\n",
        "                    exit_actual = SlippageModel.apply_slippage_to_price(exit_ideal, exit_slippage_pct, side='sell')\n",
        "                else:\n",
        "                    exit_actual = entry_actual\n",
        "\n",
        "                sell_txn = TradingCosts.calculate_transaction_cost(exit_actual, self.position_size, side='sell')\n",
        "                total_txn_cost = txn_cost + sell_txn\n",
        "\n",
        "                gross_pnl = (exit_actual - entry_actual) * self.position_size\n",
        "                net_pnl = gross_pnl - total_txn_cost\n",
        "                slippage_cost = (slippage_pct + SlippageModel.calculate_slippage(exit_ideal, 50, self.position_size)) * (entry_actual * self.position_size)\n",
        "\n",
        "                action = 'BUY'\n",
        "                entry_price_actual = entry_actual\n",
        "                exit_price_actual = exit_actual\n",
        "\n",
        "            else:  # DOWN - SHORT\n",
        "                entry_ideal = day0_close\n",
        "                entry_actual = SlippageModel.apply_slippage_to_price(entry_ideal, slippage_pct, side='sell')\n",
        "                txn_cost = TradingCosts.calculate_transaction_cost(entry_actual, self.position_size, side='sell')\n",
        "\n",
        "                if len(self.df) > 1:\n",
        "                    exit_ideal = self.df.loc[1, 'Close_SOLARINDS.NS']\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(exit_ideal, 50, self.position_size)\n",
        "                    exit_actual = SlippageModel.apply_slippage_to_price(exit_ideal, exit_slippage_pct, side='buy')\n",
        "                else:\n",
        "                    exit_actual = entry_actual\n",
        "\n",
        "                buy_txn = TradingCosts.calculate_transaction_cost(exit_actual, self.position_size, side='buy')\n",
        "                total_txn_cost = txn_cost + buy_txn\n",
        "\n",
        "                gross_pnl = (entry_actual - exit_actual) * self.position_size\n",
        "                net_pnl = gross_pnl - total_txn_cost\n",
        "                slippage_cost = (slippage_pct + SlippageModel.calculate_slippage(exit_ideal, 50, self.position_size)) * (entry_actual * self.position_size)\n",
        "\n",
        "                action = 'SHORT'\n",
        "                entry_price_actual = entry_actual\n",
        "                exit_price_actual = exit_actual\n",
        "\n",
        "            self.cumulative_pnl += net_pnl\n",
        "            self.cumulative_costs += total_txn_cost\n",
        "\n",
        "        else:\n",
        "            action = 'NO_TRADE'\n",
        "            entry_price_actual = None\n",
        "            exit_price_actual = None\n",
        "            gross_pnl = 0\n",
        "            net_pnl = 0\n",
        "            total_txn_cost = 0\n",
        "            slippage_cost = 0\n",
        "\n",
        "        # Record day 0\n",
        "        self.results['Date'].append(self.df.loc[0, 'Date'])\n",
        "        self.results['Close'].append(day0_close)\n",
        "        self.results['Confidence'].append(day0_confidence)\n",
        "        self.results['Predicted'].append(day0_signal)\n",
        "        self.results['Action'].append(action)\n",
        "        self.results['Entry_Price'].append(entry_price_actual)\n",
        "        self.results['Exit_Price'].append(exit_price_actual)\n",
        "        self.results['Position_Size'].append(self.position_size if action != 'NO_TRADE' else 0)\n",
        "        self.results['Transaction_Cost'].append(total_txn_cost)\n",
        "        self.results['Slippage_Cost'].append(slippage_cost)\n",
        "        self.results['Daily_PNL_Gross'].append(gross_pnl)\n",
        "        self.results['Daily_PNL_Net'].append(net_pnl)\n",
        "        self.results['Cumulative_PNL'].append(self.cumulative_pnl)\n",
        "        self.results['Total_Costs'].append(self.cumulative_costs)\n",
        "\n",
        "        # Days 1 onwards\n",
        "        for i in range(1, len(self.df)):\n",
        "            current_date = self.df.loc[i, 'Date']\n",
        "            current_close = self.df.loc[i, 'Close_SOLARINDS.NS']\n",
        "            current_confidence = self.df.loc[i, 'Confidence']\n",
        "            signal_class = self.df.loc[i, 'Predicted']  # 'UP', 'DOWN', 'HOLD'\n",
        "\n",
        "            # Skip low confidence or HOLD\n",
        "            if current_confidence < self.confidence_threshold or signal_class == 'HOLD':\n",
        "                action = 'NO_TRADE'\n",
        "                gross_pnl = 0\n",
        "                net_pnl = 0\n",
        "                total_txn_cost = 0\n",
        "                slippage_cost = 0\n",
        "                entry_price_actual = None\n",
        "                exit_price_actual = None\n",
        "            else:\n",
        "                prev_close = self.df.loc[i-1, 'Close_SOLARINDS.NS']\n",
        "\n",
        "                if signal_class == 'UP':  # BUY\n",
        "                    slippage_pct = SlippageModel.calculate_slippage(prev_close, 50, self.position_size)\n",
        "                    entry_actual = SlippageModel.apply_slippage_to_price(prev_close, slippage_pct, side='buy')\n",
        "                    entry_price_actual = entry_actual\n",
        "\n",
        "                    buy_txn = TradingCosts.calculate_transaction_cost(entry_actual, self.position_size, side='buy')\n",
        "\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(current_close, 50, self.position_size)\n",
        "                    exit_actual = SlippageModel.apply_slippage_to_price(current_close, exit_slippage_pct, side='sell')\n",
        "                    exit_price_actual = exit_actual\n",
        "\n",
        "                    sell_txn = TradingCosts.calculate_transaction_cost(exit_actual, self.position_size, side='sell')\n",
        "                    total_txn_cost = buy_txn + sell_txn\n",
        "                    slippage_cost = (slippage_pct + exit_slippage_pct) * (entry_actual * self.position_size)\n",
        "\n",
        "                    gross_pnl = (exit_actual - entry_actual) * self.position_size\n",
        "\n",
        "                    # Check stop loss\n",
        "                    loss_pct = (entry_actual - exit_actual) / entry_actual\n",
        "                    if loss_pct > self.stop_loss_pct:\n",
        "                        action = 'BUY_STOP_LOSS'\n",
        "                        net_pnl = -self.account_capital * self.risk_per_trade - total_txn_cost\n",
        "                    else:\n",
        "                        action = 'BUY'\n",
        "                        net_pnl = gross_pnl - total_txn_cost\n",
        "\n",
        "                else:  # DOWN - SHORT\n",
        "                    slippage_pct = SlippageModel.calculate_slippage(prev_close, 50, self.position_size)\n",
        "                    entry_actual = SlippageModel.apply_slippage_to_price(prev_close, slippage_pct, side='sell')\n",
        "                    entry_price_actual = entry_actual\n",
        "\n",
        "                    sell_txn = TradingCosts.calculate_transaction_cost(entry_actual, self.position_size, side='sell')\n",
        "\n",
        "                    exit_slippage_pct = SlippageModel.calculate_slippage(current_close, 50, self.position_size)\n",
        "                    exit_actual = SlippageModel.apply_slippage_to_price(current_close, exit_slippage_pct, side='buy')\n",
        "                    exit_price_actual = exit_actual\n",
        "\n",
        "                    buy_txn = TradingCosts.calculate_transaction_cost(exit_actual, self.position_size, side='buy')\n",
        "                    total_txn_cost = sell_txn + buy_txn\n",
        "                    slippage_cost = (slippage_pct + exit_slippage_pct) * (entry_actual * self.position_size)\n",
        "\n",
        "                    gross_pnl = (entry_actual - exit_actual) * self.position_size\n",
        "\n",
        "                    # Check stop loss\n",
        "                    loss_pct = (exit_actual - entry_actual) / entry_actual\n",
        "                    if loss_pct > self.stop_loss_pct:\n",
        "                        action = 'SHORT_STOP_LOSS'\n",
        "                        net_pnl = -self.account_capital * self.risk_per_trade - total_txn_cost\n",
        "                    else:\n",
        "                        action = 'SHORT'\n",
        "                        net_pnl = gross_pnl - total_txn_cost\n",
        "\n",
        "                self.cumulative_pnl += net_pnl\n",
        "                self.cumulative_costs += total_txn_cost\n",
        "\n",
        "            # Record\n",
        "            self.results['Date'].append(current_date)\n",
        "            self.results['Close'].append(current_close)\n",
        "            self.results['Confidence'].append(current_confidence)\n",
        "            self.results['Predicted'].append(signal_class)\n",
        "            self.results['Action'].append(action)\n",
        "            self.results['Entry_Price'].append(entry_price_actual)\n",
        "            self.results['Exit_Price'].append(exit_price_actual)\n",
        "            self.results['Position_Size'].append(self.position_size if action != 'NO_TRADE' else 0)\n",
        "            self.results['Transaction_Cost'].append(total_txn_cost)\n",
        "            self.results['Slippage_Cost'].append(slippage_cost)\n",
        "            self.results['Daily_PNL_Gross'].append(gross_pnl)\n",
        "            self.results['Daily_PNL_Net'].append(net_pnl)\n",
        "            self.results['Cumulative_PNL'].append(self.cumulative_pnl)\n",
        "            self.results['Total_Costs'].append(self.cumulative_costs)\n",
        "\n",
        "        return pd.DataFrame(self.results)\n",
        "\n",
        "    def calculate_metrics(self, results_df):\n",
        "        \"\"\"Calculate comprehensive performance metrics\"\"\"\n",
        "        initial_price = self.df.loc[0, 'Close_SOLARINDS.NS']\n",
        "        final_price = self.df.iloc[-1]['Close_SOLARINDS.NS']\n",
        "        final_pnl = results_df['Cumulative_PNL'].iloc[-1]\n",
        "        total_costs = results_df['Total_Costs'].iloc[-1]\n",
        "\n",
        "        buy_hold_return = ((final_price / initial_price) - 1) * 100\n",
        "        strategy_return = (final_pnl / self.account_capital) * 100\n",
        "\n",
        "        cumulative_max = results_df['Cumulative_PNL'].cummax()\n",
        "        drawdown = (results_df['Cumulative_PNL'] - cumulative_max) / (cumulative_max + 1e-10)\n",
        "        max_drawdown = drawdown.min() * 100\n",
        "\n",
        "        daily_pnl = results_df['Daily_PNL_Net']\n",
        "        sharpe = (daily_pnl.mean() / daily_pnl.std()) * np.sqrt(252) if daily_pnl.std() != 0 else 0\n",
        "\n",
        "        profitable = len(results_df[results_df['Daily_PNL_Net'] > 0])\n",
        "        total_trades = len(results_df[results_df['Daily_PNL_Net'] != 0])\n",
        "        win_rate = (profitable / total_trades) * 100 if total_trades > 0 else 0\n",
        "\n",
        "        metrics = {\n",
        "            'Initial_Price': initial_price,\n",
        "            'Final_Price': final_price,\n",
        "            'Buy_Hold_Return': buy_hold_return,\n",
        "            'Strategy_Return': strategy_return,\n",
        "            'Total_PNL': final_pnl,\n",
        "            'Total_Costs': total_costs,\n",
        "            'Max_Drawdown': max_drawdown,\n",
        "            'Sharpe_Ratio': sharpe,\n",
        "            'Win_Rate': win_rate,\n",
        "            'Trading_Days': total_trades,\n",
        "            'Total_Days': len(results_df)\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def print_summary(self, metrics):\n",
        "        \"\"\"Print comprehensive performance summary\"\"\"\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"REALISTIC XGBOOST BACKTEST RESULTS\")\n",
        "        print(\"=\"*70)\n",
        "        print(f\"\\nPrice Movement:\")\n",
        "        print(f\"  Initial: ₹{metrics['Initial_Price']:.2f}\")\n",
        "        print(f\"  Final: ₹{metrics['Final_Price']:.2f}\")\n",
        "        print(f\"  Buy & Hold Return: {metrics['Buy_Hold_Return']:.2f}%\")\n",
        "        print(f\"\\nStrategy Performance:\")\n",
        "        print(f\"  Total PNL: ₹{metrics['Total_PNL']:.2f}\")\n",
        "        print(f\"  Strategy Return: {metrics['Strategy_Return']:.2f}%\")\n",
        "        print(f\"  Total Costs (txn + slippage): ₹{metrics['Total_Costs']:.2f}\")\n",
        "        print(f\"\\nRisk Metrics:\")\n",
        "        print(f\"  Max Drawdown: {metrics['Max_Drawdown']:.2f}%\")\n",
        "        print(f\"  Sharpe Ratio: {metrics['Sharpe_Ratio']:.2f}\")\n",
        "        print(f\"  Win Rate: {metrics['Win_Rate']:.2f}%\")\n",
        "        print(f\"\\nTrading Activity:\")\n",
        "        print(f\"  Trading Days: {metrics['Trading_Days']} / {metrics['Total_Days']}\")\n",
        "        print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    def export_results(self, results_df, csv_path='xgb_backtest_realistic.csv'):\n",
        "        \"\"\"Export detailed results\"\"\"\n",
        "        results_df.to_csv(csv_path, index=False)\n",
        "        print(f\"Results exported to '{csv_path}'\")\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    backtest = RealisticBacktestingXGBoost(\n",
        "        probability_file_path='XGB_Probability_Scores_Sorted.csv',\n",
        "        confidence_threshold=0.60,\n",
        "        stop_loss_pct=0.02,\n",
        "        take_profit_pct=0.05,\n",
        "        account_capital=100000,\n",
        "        risk_per_trade=0.02\n",
        "    )\n",
        "\n",
        "    print(\"\\nExecuting realistic XGBoost backtest...\\n\")\n",
        "    results_df = backtest.execute_backtest()\n",
        "\n",
        "    metrics = backtest.calculate_metrics(results_df)\n",
        "    backtest.print_summary(metrics)\n",
        "\n",
        "    backtest.export_results(results_df)\n",
        "\n",
        "    print(\"\\nAction Distribution:\")\n",
        "    print(results_df['Action'].value_counts())\n",
        "\n",
        "    print(\"\\nTrading Days Summary (first 20):\")\n",
        "    trading_days = results_df[results_df['Daily_PNL_Net'] != 0]\n",
        "    print(trading_days[['Date', 'Action', 'Entry_Price', 'Exit_Price',\n",
        "                         'Daily_PNL_Gross', 'Transaction_Cost', 'Daily_PNL_Net']].head(20))\n",
        "\n",
        "    print(\"\\nCost Analysis:\")\n",
        "    print(f\"Total Transaction Costs: ₹{results_df['Transaction_Cost'].sum():.2f}\")\n",
        "    print(f\"Total Slippage Costs: ₹{results_df['Slippage_Cost'].sum():.2f}\")\n",
        "    print(f\"Combined Cost Impact: {(results_df['Transaction_Cost'].sum() + results_df['Slippage_Cost'].sum()) / abs(results_df['Daily_PNL_Gross'].sum()) * 100:.2f}% of gross PNL\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mf5TUIas4hdB",
        "outputId": "1e4bae78-4c3b-48e7-e858-a20f121ba0f0"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "REALISTIC XGBOOST BACKTEST CONFIGURATION\n",
            "======================================================================\n",
            "Confidence Threshold: 60%\n",
            "Stop Loss: 2.0%\n",
            "Take Profit: 5.0%\n",
            "Account Capital: ₹100,000\n",
            "Risk Per Trade: 2.0%\n",
            "Position Size: 100 shares\n",
            "======================================================================\n",
            "\n",
            "\n",
            "Executing realistic XGBoost backtest...\n",
            "\n",
            "\n",
            "======================================================================\n",
            "REALISTIC XGBOOST BACKTEST RESULTS\n",
            "======================================================================\n",
            "\n",
            "Price Movement:\n",
            "  Initial: ₹1080.37\n",
            "  Final: ₹13901.00\n",
            "  Buy & Hold Return: 1186.69%\n",
            "\n",
            "Strategy Performance:\n",
            "  Total PNL: ₹4568589.49\n",
            "  Strategy Return: 4568.59%\n",
            "  Total Costs (txn + slippage): ₹325800.44\n",
            "\n",
            "Risk Metrics:\n",
            "  Max Drawdown: -13.87%\n",
            "  Sharpe Ratio: 4.89\n",
            "  Win Rate: 75.81%\n",
            "\n",
            "Trading Activity:\n",
            "  Trading Days: 583 / 1428\n",
            "======================================================================\n",
            "\n",
            "Results exported to 'xgb_backtest_realistic.csv'\n",
            "\n",
            "Action Distribution:\n",
            "Action\n",
            "NO_TRADE           845\n",
            "BUY                348\n",
            "SHORT              215\n",
            "BUY_STOP_LOSS       12\n",
            "SHORT_STOP_LOSS      8\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Trading Days Summary (first 20):\n",
            "         Date Action  Entry_Price   Exit_Price  Daily_PNL_Gross  \\\n",
            "1  2020-01-03    BUY  1083.069941  1098.491639      1542.169800   \n",
            "2  2020-01-06  SHORT  1098.491639  1099.407436       -91.579681   \n",
            "3  2020-01-07  SHORT  1093.924107  1092.497382       142.672546   \n",
            "16 2020-01-24    BUY  1136.228874  1158.997831      2276.895721   \n",
            "17 2020-01-27    BUY  1164.807344  1176.530793      1172.344910   \n",
            "18 2020-01-28    BUY  1182.428191  1201.676268      1924.807739   \n",
            "19 2020-01-29    BUY  1207.699708  1250.248138      4254.842957   \n",
            "20 2020-01-30    BUY  1256.515046  1271.464617      1494.957123   \n",
            "21 2020-01-31  SHORT  1271.464617  1250.937538      2052.707886   \n",
            "22 2020-02-03    BUY  1250.937538  1258.007986       707.044830   \n",
            "25 2020-02-06    BUY  1249.654309  1255.748878       609.456909   \n",
            "26 2020-02-07    BUY  1262.043358  1257.811458      -423.190094   \n",
            "29 2020-02-12  SHORT  1215.378621  1195.853761      1952.485992   \n",
            "30 2020-02-13  SHORT  1189.889403  1170.631561      1925.784210   \n",
            "32 2020-02-17  SHORT  1158.801424  1150.049228       875.219666   \n",
            "34 2020-02-19    BUY  1160.068843  1226.035749      6596.690613   \n",
            "46 2020-03-09  SHORT  1112.390399  1098.617869      1377.252960   \n",
            "48 2020-03-12  SHORT  1099.130175  1023.741142      7538.903366   \n",
            "50 2020-03-16  SHORT  1052.669957  1026.357101      2631.285553   \n",
            "52 2020-03-18  SHORT  1002.133651   992.694725       943.892532   \n",
            "\n",
            "    Transaction_Cost  Daily_PNL_Net  \n",
            "1         106.871132    1435.298667  \n",
            "2         107.465817    -199.045499  \n",
            "3         106.933845      35.738702  \n",
            "16        112.521198    2164.374524  \n",
            "17        114.637978    1057.706932  \n",
            "18        116.823309    1807.984430  \n",
            "19        120.725505    4134.117452  \n",
            "20        123.805075    1371.152048  \n",
            "21        123.602054    1929.105832  \n",
            "22        122.775817     584.269014  \n",
            "25        122.590398     486.866511  \n",
            "26        123.168002    -546.358096  \n",
            "29        118.153324    1834.332668  \n",
            "30        115.670198    1810.114012  \n",
            "32        113.012199     762.207466  \n",
            "34        117.505101    6479.185512  \n",
            "46        108.290461    1268.962499  \n",
            "48        104.750770    7434.152596  \n",
            "50        101.993334    2529.292219  \n",
            "52         97.665094     846.227438  \n",
            "\n",
            "Cost Analysis:\n",
            "Total Transaction Costs: ₹325800.44\n",
            "Total Slippage Costs: ₹1660351.55\n",
            "Combined Cost Impact: 44.67% of gross PNL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Separate XGBoost models for high , medium and low market volatility"
      ],
      "metadata": {
        "id": "LxwDDRq0WHgu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High Volatility"
      ],
      "metadata": {
        "id": "yctOALqvUHj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# --- Step 1: Copy dataframe ---\n",
        "hdf = high_vol_df.copy()\n",
        "\n",
        "# --- Step 2: Encode categorical columns ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    hdf[col] = le.fit_transform(hdf[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 3: Split into train (<=2019) and test (>=2020) using 'Year' ---\n",
        "train_df = hdf[hdf['Year'] <= 2019].copy()\n",
        "test_df  = hdf[hdf['Year'] >= 2020].copy()\n",
        "\n",
        "# --- Step 4: Separate features and target ---\n",
        "X_train = train_df.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year']) # Drop Date and Year\n",
        "y_train = train_df['Direction']\n",
        "X_test  = test_df.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year']) # Drop Date and Year\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# --- Step 5: Scale numeric features ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# --- Step 6: Define and train XGBoost model ---\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(label_encoders['Direction'].classes_),\n",
        "    eval_metric='mlogloss',\n",
        "    learning_rate=0.1,\n",
        "    max_depth=7,\n",
        "    n_estimators=700,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=17\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 7: Feature Importance ---\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nXGBoost Feature Importances:\")\n",
        "print(importance_df)\n",
        "\n",
        "# --- Step 8: Predictions & probability scores ---\n",
        "y_pred = xgb_model.predict(X_test_scaled)\n",
        "y_proba = xgb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 9: Build probability dataframe ---\n",
        "proba_df_high = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in xgb_model.classes_]\n",
        ")\n",
        "proba_df_high[\"Predicted\"] = y_pred\n",
        "proba_df_high[\"Confidence\"] = proba_df_high[\n",
        "    [f\"P_{cls}\" for cls in xgb_model.classes_]\n",
        "].max(axis=1)\n",
        "\n",
        "# Add Year and Close price for inspection\n",
        "proba_df_high[\"Year\"] = test_df[\"Year\"].values\n",
        "proba_df_high[\"Close_SOLARINDS.NS\"] = test_df[\"Close_SOLARINDS.NS\"].values\n",
        "\n",
        "# --- Step 10: Display probability scores ---\n",
        "print(\"\\nProbability scores (first 10 samples):\")\n",
        "print(proba_df_high.head(10))\n",
        "\n",
        "# --- Step 11: Evaluation ---\n",
        "print(\"\\nModel Evaluation (XGBoost):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))"
      ],
      "metadata": {
        "id": "1EBzjcR2fpdU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12d11d63-8aff-4735-b833-5f294322c76b"
      },
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost Feature Importances:\n",
            "             Feature  Importance\n",
            "10       EMA_Compare    0.198295\n",
            "11      EMA_Diff_Pct    0.112841\n",
            "7                RSI    0.097428\n",
            "8           RSI_Avg3    0.084544\n",
            "5   Lag_Weighted_Avg    0.081736\n",
            "2               Lag3    0.069239\n",
            "1               Lag2    0.062609\n",
            "0               Lag1    0.061205\n",
            "6             Volume    0.060955\n",
            "3               Lag4    0.060422\n",
            "4               Lag5    0.058987\n",
            "9                EMA    0.051740\n",
            "\n",
            "Probability scores (first 10 samples):\n",
            "        P_0       P_1       P_2  Predicted  Confidence  Year  \\\n",
            "0  0.067712  0.070022  0.862267          2    0.862267  2020   \n",
            "1  0.214447  0.146121  0.639432          2    0.639432  2020   \n",
            "2  0.627992  0.049752  0.322256          0    0.627992  2020   \n",
            "3  0.010442  0.343992  0.645567          2    0.645567  2020   \n",
            "4  0.044725  0.928229  0.027047          1    0.928229  2020   \n",
            "5  0.195485  0.803737  0.000778          1    0.803737  2020   \n",
            "6  0.363281  0.614960  0.021759          1    0.614960  2020   \n",
            "7  0.492451  0.506580  0.000969          1    0.506580  2020   \n",
            "8  0.382003  0.290307  0.327690          0    0.382003  2020   \n",
            "9  0.787609  0.201783  0.010608          0    0.787609  2020   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1130.145874  \n",
            "1         1122.120483  \n",
            "2         1112.962646  \n",
            "3         1121.578857  \n",
            "4         1115.178345  \n",
            "5         1095.878174  \n",
            "6         1101.884888  \n",
            "7         1021.188171  \n",
            "8         1055.308228  \n",
            "9         1023.797607  \n",
            "\n",
            "Model Evaluation (XGBoost):\n",
            "[[ 46  43   9]\n",
            " [ 30 121  61]\n",
            " [  4  32  74]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.57      0.47      0.52        98\n",
            "        HOLD       0.62      0.57      0.59       212\n",
            "          UP       0.51      0.67      0.58       110\n",
            "\n",
            "    accuracy                           0.57       420\n",
            "   macro avg       0.57      0.57      0.56       420\n",
            "weighted avg       0.58      0.57      0.57       420\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Medium Volaitlity"
      ],
      "metadata": {
        "id": "UyIGBAM2ULIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# --- Step 1: Copy dataframe ---\n",
        "mdf = med_vol_df.copy()\n",
        "\n",
        "# --- Step 2: Encode categorical columns ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    mdf[col] = le.fit_transform(mdf[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 3: Split into train (<=2019) and test (>=2020) using 'Year' ---\n",
        "train_df = mdf[mdf['Year'] <= 2019].copy()\n",
        "test_df  = mdf[mdf['Year'] >= 2020].copy()\n",
        "\n",
        "# --- Step 4: Separate features and target ---\n",
        "X_train = train_df.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year']) # Drop Date and Year\n",
        "y_train = train_df['Direction']\n",
        "X_test  = test_df.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year']) # Drop Date and Year\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# --- Step 5: Scale numeric features ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# --- Step 6: Define and train XGBoost model ---\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(label_encoders['Direction'].classes_),\n",
        "    eval_metric='mlogloss',\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    n_estimators=700,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=17\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 7: Feature Importance ---\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nXGBoost Feature Importances:\")\n",
        "print(importance_df)\n",
        "\n",
        "# --- Step 8: Predictions & probability scores ---\n",
        "y_pred = xgb_model.predict(X_test_scaled)\n",
        "y_proba = xgb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 9: Build probability dataframe ---\n",
        "proba_df_medium = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in xgb_model.classes_]\n",
        ")\n",
        "proba_df_medium[\"Predicted\"] = y_pred\n",
        "proba_df_medium[\"Confidence\"] = proba_df_medium[\n",
        "    [f\"P_{cls}\" for cls in xgb_model.classes_]\n",
        "].max(axis=1)\n",
        "\n",
        "# Add Year and Close price for inspection\n",
        "proba_df_medium[\"Year\"] = test_df[\"Year\"].values\n",
        "proba_df_medium[\"Close_SOLARINDS.NS\"] = test_df[\"Close_SOLARINDS.NS\"].values\n",
        "\n",
        "# --- Step 10: Display probability scores ---\n",
        "print(\"\\nProbability scores (first 10 samples):\")\n",
        "print(proba_df_medium.head(10))\n",
        "\n",
        "# --- Step 11: Evaluation ---\n",
        "print(\"\\nModel Evaluation (XGBoost):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68FjB88VSEi-",
        "outputId": "21216d1e-fe3e-4640-fe0f-22ae67fa6b7b"
      },
      "execution_count": 253,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost Feature Importances:\n",
            "             Feature  Importance\n",
            "10       EMA_Compare    0.190488\n",
            "11      EMA_Diff_Pct    0.131309\n",
            "7                RSI    0.108839\n",
            "5   Lag_Weighted_Avg    0.089006\n",
            "8           RSI_Avg3    0.076159\n",
            "0               Lag1    0.065061\n",
            "6             Volume    0.063638\n",
            "3               Lag4    0.056388\n",
            "1               Lag2    0.056052\n",
            "2               Lag3    0.054708\n",
            "4               Lag5    0.054207\n",
            "9                EMA    0.054147\n",
            "\n",
            "Probability scores (first 10 samples):\n",
            "        P_0       P_1       P_2  Predicted  Confidence  Year  \\\n",
            "0  0.986215  0.013312  0.000473          0    0.986215  2020   \n",
            "1  0.189588  0.800345  0.010067          1    0.800345  2020   \n",
            "2  0.009264  0.988267  0.002469          1    0.988267  2020   \n",
            "3  0.077394  0.891086  0.031520          1    0.891086  2020   \n",
            "4  0.344501  0.649419  0.006080          1    0.649419  2020   \n",
            "5  0.274906  0.720396  0.004698          1    0.720396  2020   \n",
            "6  0.003990  0.014719  0.981291          2    0.981291  2020   \n",
            "7  0.019435  0.798761  0.181804          1    0.798761  2020   \n",
            "8  0.090734  0.886532  0.022734          1    0.886532  2020   \n",
            "9  0.142865  0.296890  0.560245          2    0.560245  2020   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1089.772949  \n",
            "1         1087.114258  \n",
            "2         1094.548706  \n",
            "3         1143.094727  \n",
            "4         1143.882568  \n",
            "5         1133.395386  \n",
            "6         1161.902588  \n",
            "7         1179.479492  \n",
            "8         1204.687988  \n",
            "9         1253.381592  \n",
            "\n",
            "Model Evaluation (XGBoost):\n",
            "[[64 38 10]\n",
            " [45 89 47]\n",
            " [12 42 78]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.53      0.57      0.55       112\n",
            "        HOLD       0.53      0.49      0.51       181\n",
            "          UP       0.58      0.59      0.58       132\n",
            "\n",
            "    accuracy                           0.54       425\n",
            "   macro avg       0.54      0.55      0.55       425\n",
            "weighted avg       0.54      0.54      0.54       425\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Low Volatility"
      ],
      "metadata": {
        "id": "b3ilHbPSURlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# --- Step 1: Copy dataframe ---\n",
        "ldf = low_vol_df.copy()\n",
        "\n",
        "# --- Step 2: Encode categorical columns ---\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    ldf[col] = le.fit_transform(ldf[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# --- Step 3: Split into train (<=2019) and test (>=2020) using 'Year' ---\n",
        "train_df = ldf[ldf['Year'] <= 2019].copy()\n",
        "test_df  = ldf[ldf['Year'] >= 2020].copy()\n",
        "\n",
        "# --- Step 4: Separate features and target ---\n",
        "X_train = train_df.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year']) # Drop Date and Year\n",
        "y_train = train_df['Direction']\n",
        "X_test  = test_df.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year']) # Drop Date and Year\n",
        "y_test  = test_df['Direction']\n",
        "\n",
        "# --- Step 5: Scale numeric features ---\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "# --- Step 6: Define and train XGBoost model ---\n",
        "xgb_model = XGBClassifier(\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(label_encoders['Direction'].classes_),\n",
        "    eval_metric='mlogloss',\n",
        "    learning_rate=0.05,\n",
        "    max_depth=7,\n",
        "    n_estimators=700,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=17\n",
        ")\n",
        "\n",
        "xgb_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# --- Step 7: Feature Importance ---\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': xgb_model.feature_importances_\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"\\nXGBoost Feature Importances:\")\n",
        "print(importance_df)\n",
        "\n",
        "# --- Step 8: Predictions & probability scores ---\n",
        "y_pred = xgb_model.predict(X_test_scaled)\n",
        "y_proba = xgb_model.predict_proba(X_test_scaled)\n",
        "\n",
        "# --- Step 9: Build probability dataframe ---\n",
        "proba_df_low = pd.DataFrame(\n",
        "    y_proba,\n",
        "    columns=[f\"P_{cls}\" for cls in xgb_model.classes_]\n",
        ")\n",
        "proba_df_low[\"Predicted\"] = y_pred\n",
        "proba_df_low[\"Confidence\"] = proba_df_low[\n",
        "    [f\"P_{cls}\" for cls in xgb_model.classes_]\n",
        "].max(axis=1)\n",
        "\n",
        "# Add Year and Close price for inspection\n",
        "proba_df_low[\"Year\"] = test_df[\"Year\"].values\n",
        "proba_df_low[\"Close_SOLARINDS.NS\"] = test_df[\"Close_SOLARINDS.NS\"].values\n",
        "\n",
        "# --- Step 10: Display probability scores ---\n",
        "print(\"\\nProbability scores (first 10 samples):\")\n",
        "print(proba_df_low.head(10))\n",
        "\n",
        "# --- Step 11: Evaluation ---\n",
        "print(\"\\nModel Evaluation (XGBoost):\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoders['Direction'].classes_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyfzaaBnTMw6",
        "outputId": "daba6500-3fe8-4b2c-e59d-77f77954b1d7"
      },
      "execution_count": 254,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "XGBoost Feature Importances:\n",
            "             Feature  Importance\n",
            "10       EMA_Compare    0.137673\n",
            "11      EMA_Diff_Pct    0.119243\n",
            "7                RSI    0.107243\n",
            "5   Lag_Weighted_Avg    0.087006\n",
            "8           RSI_Avg3    0.081414\n",
            "6             Volume    0.071376\n",
            "4               Lag5    0.071114\n",
            "9                EMA    0.069374\n",
            "0               Lag1    0.066305\n",
            "1               Lag2    0.064750\n",
            "3               Lag4    0.062515\n",
            "2               Lag3    0.061987\n",
            "\n",
            "Probability scores (first 10 samples):\n",
            "        P_0       P_1       P_2  Predicted  Confidence  Year  \\\n",
            "0  0.000123  0.507574  0.492303          1    0.507574  2020   \n",
            "1  0.012412  0.033239  0.954349          2    0.954349  2020   \n",
            "2  0.949352  0.047876  0.002772          0    0.949352  2020   \n",
            "3  0.118182  0.848623  0.033194          1    0.848623  2020   \n",
            "4  0.018950  0.949551  0.031500          1    0.949551  2020   \n",
            "5  0.002593  0.961535  0.035872          1    0.961535  2020   \n",
            "6  0.000708  0.960895  0.038397          1    0.960895  2020   \n",
            "7  0.075522  0.918852  0.005626          1    0.918852  2020   \n",
            "8  0.006739  0.987265  0.005996          1    0.987265  2020   \n",
            "9  0.051231  0.767150  0.181618          1    0.767150  2020   \n",
            "\n",
            "   Close_SOLARINDS.NS  \n",
            "0         1080.369019  \n",
            "1         1101.244751  \n",
            "2         1096.665771  \n",
            "3         1085.243164  \n",
            "4         1080.122803  \n",
            "5         1079.728882  \n",
            "6         1102.623291  \n",
            "7         1103.361938  \n",
            "8         1117.541748  \n",
            "9         1127.684204  \n",
            "\n",
            "Model Evaluation (XGBoost):\n",
            "[[ 48  80  17]\n",
            " [ 26 218  48]\n",
            " [  2  59  85]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        DOWN       0.63      0.33      0.43       145\n",
            "        HOLD       0.61      0.75      0.67       292\n",
            "          UP       0.57      0.58      0.57       146\n",
            "\n",
            "    accuracy                           0.60       583\n",
            "   macro avg       0.60      0.55      0.56       583\n",
            "weighted avg       0.60      0.60      0.59       583\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "wY0tCCiBjaeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, LabelEncoder"
      ],
      "metadata": {
        "id": "8IAOakPqjhVb"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "\n",
        "df = final_df.copy()\n",
        "df['Date'] = pd.to_datetime(final_df['Date'])\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Volatility_Regime', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "nW1JlJOZkCXw"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(columns=['Direction', 'Volatility_Regime', 'Close_SOLARINDS.NS', 'Date', 'Year'])\n",
        "y = df['Direction']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n"
      ],
      "metadata": {
        "id": "2RmEaHBxkFHf"
      },
      "execution_count": 257,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy arrays\n",
        "X_scaled = np.array(X_scaled)\n",
        "y = np.array(y)\n",
        "\n",
        "window_size = 50  # or any value you decide\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for i in range(window_size, len(X_scaled)):\n",
        "    X_seq.append(X_scaled[i - window_size:i])\n",
        "    y_seq.append(y[i])\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n",
        "\n",
        "print(f\"X_seq shape: {X_seq.shape}\")  # (samples, window_size, num_features)\n",
        "print(f\"y_seq shape: {y_seq.shape}\")  # (samples,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKmu8495legm",
        "outputId": "8a5bb2da-4b32-4b8b-af17-ce2202d917ad"
      },
      "execution_count": 258,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq shape: (3820, 50, 13)\n",
            "y_seq shape: (3820,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get corresponding dates for each sequence\n",
        "dates = df['Date'][window_size:].reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "1_dOf4sClgkO"
      },
      "execution_count": 259,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create boolean masks\n",
        "train_mask = dates < '2020-01-01'\n",
        "test_mask = (dates >= '2020-01-01') & (dates <= '2025-12-31')\n",
        "\n",
        "# Apply masks\n",
        "X_train = X_seq[train_mask.values]\n",
        "y_train = y_seq[train_mask.values]\n",
        "X_test = X_seq[test_mask.values]\n",
        "y_test = y_seq[test_mask.values]\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "print(\"Train range:\", dates[train_mask].min(), \"→\", dates[train_mask].max())\n",
        "print(\"Test range:\", dates[test_mask].min(), \"→\", dates[test_mask].max())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCUXNzvmljOI",
        "outputId": "b94f311c-376f-4be2-b0ef-3d03c09fb6c2"
      },
      "execution_count": 260,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (2392, 50, 13) Test size: (1428, 50, 13)\n",
            "Train range: 2010-04-15 00:00:00 → 2019-12-31 00:00:00\n",
            "Test range: 2020-01-02 00:00:00 → 2025-10-30 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: check class balance or visualize\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Train class distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(\"Test class distribution:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFebaqfkllGd",
        "outputId": "e582ee3a-ff95-488d-c87a-ecfbe8068f9f"
      },
      "execution_count": 261,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution: {np.int64(0): np.int64(480), np.int64(1): np.int64(1381), np.int64(2): np.int64(531)}\n",
            "Test class distribution: {np.int64(0): np.int64(355), np.int64(1): np.int64(685), np.int64(2): np.int64(388)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled = X_train\n",
        "y_resampled = y_train"
      ],
      "metadata": {
        "id": "CYqiJ_sklnNh"
      },
      "execution_count": 262,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout"
      ],
      "metadata": {
        "id": "7E8_mWv5lpxz"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5jpJtSWlrvi",
        "outputId": "8fc63354-9f2a-4933-d64a-f8fcf2547450"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2392, 50, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (window_size, X_resampled.shape[2])\n",
        "num_classes = 3 #buy, hold, sell\n",
        "\n",
        "input_shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LBqrgKHlvgV",
        "outputId": "25d0de27-f619-4d52-ec81-83fbce15dcc7"
      },
      "execution_count": 265,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 13)"
            ]
          },
          "metadata": {},
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#complex model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Bidirectional(\n",
        "        layers.LSTM(\n",
        "            64, return_sequences=True,\n",
        "            dropout=0.3, recurrent_dropout=0.2,\n",
        "            kernel_regularizer=regularizers.l2(1e-4),\n",
        "            recurrent_regularizer=regularizers.l2(1e-4)\n",
        "        ),\n",
        "        input_shape=(60, X_seq.shape[2])\n",
        "    ),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LSTM(\n",
        "        64,\n",
        "        dropout=0.3, recurrent_dropout=0.2,\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkeKmbYKlyfH",
        "outputId": "ad9bf780-906d-4cb1-a28d-ce4ad0a8360e"
      },
      "execution_count": 266,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF7NlfZYl0a2",
        "outputId": "d37852c6-8391-4f0c-8e53-a1562058f27c"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 222ms/step - accuracy: 0.5137 - loss: 1.0591 - val_accuracy: 0.4797 - val_loss: 1.0943\n",
            "Epoch 2/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 199ms/step - accuracy: 0.5766 - loss: 1.0282 - val_accuracy: 0.4797 - val_loss: 1.0919\n",
            "Epoch 3/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 195ms/step - accuracy: 0.5785 - loss: 1.0041 - val_accuracy: 0.4797 - val_loss: 1.1017\n",
            "Epoch 4/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 184ms/step - accuracy: 0.5771 - loss: 1.0028 - val_accuracy: 0.4804 - val_loss: 1.1103\n",
            "Epoch 5/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 183ms/step - accuracy: 0.5914 - loss: 0.9747 - val_accuracy: 0.4790 - val_loss: 1.1157\n",
            "Epoch 6/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 182ms/step - accuracy: 0.5713 - loss: 1.0025 - val_accuracy: 0.4797 - val_loss: 1.0903\n",
            "Epoch 7/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 191ms/step - accuracy: 0.5609 - loss: 1.0041 - val_accuracy: 0.4790 - val_loss: 1.0815\n",
            "Epoch 8/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 185ms/step - accuracy: 0.5876 - loss: 0.9860 - val_accuracy: 0.4741 - val_loss: 1.0864\n",
            "Epoch 9/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 188ms/step - accuracy: 0.5701 - loss: 0.9950 - val_accuracy: 0.4797 - val_loss: 1.0891\n",
            "Epoch 10/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 193ms/step - accuracy: 0.5722 - loss: 1.0030 - val_accuracy: 0.4776 - val_loss: 1.0840\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa26740dc40>"
            ]
          },
          "metadata": {},
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RMcd1X6Xl2hm",
        "outputId": "ab414f64-d52e-4fd7-d35f-c040ae9db6f7"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 59ms/step\n",
            "[[  0 349   6]\n",
            " [  1 678   6]\n",
            " [  1 383   4]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       355\n",
            "           1       0.48      0.99      0.65       685\n",
            "           2       0.25      0.01      0.02       388\n",
            "\n",
            "    accuracy                           0.48      1428\n",
            "   macro avg       0.24      0.33      0.22      1428\n",
            "weighted avg       0.30      0.48      0.32      1428\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple model\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=input_shape),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model1.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw_ptmyZl6k9",
        "outputId": "5c0dcdca-c93b-4898-a3e4-c1ca826596b8"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nx-QV_oul8SD",
        "outputId": "6ef0601c-934a-44d2-948d-d6dd59dd4014"
      },
      "execution_count": 270,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 86ms/step - accuracy: 0.5047 - loss: 1.0252 - val_accuracy: 0.4356 - val_loss: 1.1089\n",
            "Epoch 2/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 74ms/step - accuracy: 0.5677 - loss: 0.9783 - val_accuracy: 0.4426 - val_loss: 1.1197\n",
            "Epoch 3/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.5703 - loss: 0.9652 - val_accuracy: 0.4202 - val_loss: 1.1410\n",
            "Epoch 4/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.5875 - loss: 0.9425 - val_accuracy: 0.4412 - val_loss: 1.1045\n",
            "Epoch 5/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.5724 - loss: 0.9525 - val_accuracy: 0.4377 - val_loss: 1.1132\n",
            "Epoch 6/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 100ms/step - accuracy: 0.5728 - loss: 0.9556 - val_accuracy: 0.4104 - val_loss: 1.1182\n",
            "Epoch 7/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 103ms/step - accuracy: 0.5675 - loss: 0.9566 - val_accuracy: 0.4419 - val_loss: 1.0917\n",
            "Epoch 8/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 90ms/step - accuracy: 0.5902 - loss: 0.9335 - val_accuracy: 0.4447 - val_loss: 1.0957\n",
            "Epoch 9/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 70ms/step - accuracy: 0.5791 - loss: 0.9489 - val_accuracy: 0.4426 - val_loss: 1.0960\n",
            "Epoch 10/10\n",
            "\u001b[1m75/75\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 88ms/step - accuracy: 0.6037 - loss: 0.9029 - val_accuracy: 0.4370 - val_loss: 1.0918\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa260e09400>"
            ]
          },
          "metadata": {},
          "execution_count": 270
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model1.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4_gjwK6l-RI",
        "outputId": "cced95d6-96ea-47bc-b60e-fe5863e600fe"
      },
      "execution_count": 271,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step\n",
            "[[ 84 179  92]\n",
            " [124 439 122]\n",
            " [ 79 208 101]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.29      0.24      0.26       355\n",
            "           1       0.53      0.64      0.58       685\n",
            "           2       0.32      0.26      0.29       388\n",
            "\n",
            "    accuracy                           0.44      1428\n",
            "   macro avg       0.38      0.38      0.38      1428\n",
            "weighted avg       0.41      0.44      0.42      1428\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Separate models for high, medium & low market volatility"
      ],
      "metadata": {
        "id": "aSxwnJCQohk1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## High Volatility"
      ],
      "metadata": {
        "id": "Ez8f69Ozp9gt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "\n",
        "hdf = hdf.copy()\n",
        "hdf['Date'] = pd.to_datetime(hdf['Date'])\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Volatility_Regime', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "NWdomDY3oiLF"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = hdf.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year'])\n",
        "y = hdf['Direction']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "oIHXmZJdooQ2"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy arrays\n",
        "X_scaled = np.array(X_scaled)\n",
        "y = np.array(y)\n",
        "\n",
        "window_size = 50  # or any value you decide\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for i in range(window_size, len(X_scaled)):\n",
        "    X_seq.append(X_scaled[i - window_size:i])\n",
        "    y_seq.append(y[i])\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n",
        "\n",
        "print(f\"X_seq shape: {X_seq.shape}\")  # (samples, window_size, num_features)\n",
        "print(f\"y_seq shape: {y_seq.shape}\")  # (samples,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLtY4XzGotBz",
        "outputId": "0354f849-a2f5-4aa2-cf6d-5200368d6e71"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq shape: (1099, 50, 12)\n",
            "y_seq shape: (1099,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get corresponding dates for each sequence\n",
        "dates = hdf['Date'][window_size:].reset_index(drop=True) # Use hdf instead of df\n",
        "\n",
        "# Create boolean masks\n",
        "train_mask = dates < '2020-01-01'\n",
        "test_mask = (dates >= '2020-01-01') & (dates <= '2025-12-31')\n",
        "\n",
        "# Apply masks\n",
        "X_train = X_seq[train_mask.values]\n",
        "y_train = y_seq[train_mask.values]\n",
        "X_test = X_seq[test_mask.values]\n",
        "y_test = y_seq[test_mask.values]\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "print(\"Train range:\", dates[train_mask].min(), \"→\", dates[train_mask].max())\n",
        "print(\"Test range:\", dates[test_mask].min(), \"→\", dates[test_mask].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A48GIqdro-xW",
        "outputId": "939723fc-0fbc-4db2-a7cb-76283456cbaa"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (679, 50, 12) Test size: (420, 50, 12)\n",
            "Train range: 2010-05-13 00:00:00 → 2019-05-24 00:00:00\n",
            "Test range: 2020-03-02 00:00:00 → 2025-05-12 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: check class balance or visualize\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Train class distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(\"Test class distribution:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWXuqGedpBvz",
        "outputId": "7edb940e-1e31-4b05-dfec-a75bac215b14"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution: {np.int64(0): np.int64(134), np.int64(1): np.int64(367), np.int64(2): np.int64(178)}\n",
            "Test class distribution: {np.int64(0): np.int64(98), np.int64(1): np.int64(212), np.int64(2): np.int64(110)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_resampled = X_train\n",
        "y_resampled = y_train"
      ],
      "metadata": {
        "id": "7ZGKeQWxpZlu"
      },
      "execution_count": 277,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#complex model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Bidirectional(\n",
        "        layers.LSTM(\n",
        "            64, return_sequences=True,\n",
        "            dropout=0.3, recurrent_dropout=0.2,\n",
        "            kernel_regularizer=regularizers.l2(1e-4),\n",
        "            recurrent_regularizer=regularizers.l2(1e-4)\n",
        "        ),\n",
        "        input_shape=(60, X_seq.shape[2])\n",
        "    ),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LSTM(\n",
        "        64,\n",
        "        dropout=0.3, recurrent_dropout=0.2,\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8N62LeH1pcaf",
        "outputId": "9ce02eda-f5ae-4788-d31e-48a0d909c8d9"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoKCSctJpf4x",
        "outputId": "09715da5-86e6-4ba1-abcd-c17f98fef82b"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 241ms/step - accuracy: 0.3765 - loss: 1.1444 - val_accuracy: 0.5048 - val_loss: 1.0878\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 177ms/step - accuracy: 0.5192 - loss: 1.0744 - val_accuracy: 0.5119 - val_loss: 1.0839\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 180ms/step - accuracy: 0.5144 - loss: 1.0640 - val_accuracy: 0.5095 - val_loss: 1.0798\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.5428 - loss: 1.0201 - val_accuracy: 0.5071 - val_loss: 1.0751\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.5230 - loss: 1.0423 - val_accuracy: 0.5071 - val_loss: 1.0750\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.5079 - loss: 1.0634 - val_accuracy: 0.5071 - val_loss: 1.0673\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 208ms/step - accuracy: 0.5447 - loss: 1.0283 - val_accuracy: 0.5048 - val_loss: 1.0668\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 181ms/step - accuracy: 0.5373 - loss: 1.0126 - val_accuracy: 0.4952 - val_loss: 1.0596\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.5326 - loss: 1.0365 - val_accuracy: 0.4952 - val_loss: 1.0604\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 258ms/step - accuracy: 0.5582 - loss: 0.9938 - val_accuracy: 0.4905 - val_loss: 1.0592\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa27f851400>"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1vFMSJHpkBH",
        "outputId": "a2fe40d8-827b-435d-a175-e9ca6abe422b"
      },
      "execution_count": 280,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 184ms/step - accuracy: 0.5234 - loss: 1.0213 - val_accuracy: 0.4881 - val_loss: 1.0556\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 170ms/step - accuracy: 0.5352 - loss: 1.0159 - val_accuracy: 0.4952 - val_loss: 1.0690\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 205ms/step - accuracy: 0.5311 - loss: 1.0180 - val_accuracy: 0.5048 - val_loss: 1.0695\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 168ms/step - accuracy: 0.5497 - loss: 1.0038 - val_accuracy: 0.4976 - val_loss: 1.0731\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 215ms/step - accuracy: 0.5542 - loss: 1.0183 - val_accuracy: 0.4952 - val_loss: 1.0487\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 190ms/step - accuracy: 0.5230 - loss: 1.0344 - val_accuracy: 0.4952 - val_loss: 1.0544\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 177ms/step - accuracy: 0.5507 - loss: 1.0035 - val_accuracy: 0.5048 - val_loss: 1.0562\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 203ms/step - accuracy: 0.5477 - loss: 1.0003 - val_accuracy: 0.5095 - val_loss: 1.0656\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 178ms/step - accuracy: 0.5719 - loss: 0.9887 - val_accuracy: 0.4976 - val_loss: 1.0679\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 166ms/step - accuracy: 0.5725 - loss: 0.9649 - val_accuracy: 0.4738 - val_loss: 1.0750\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa2ff7ea4b0>"
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsTRBn5dqdbt",
        "outputId": "d3cf9288-010c-4554-c45f-c4ba5ce4381d"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step\n",
            "[[  6  64  28]\n",
            " [  6 162  44]\n",
            " [  6  73  31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.06      0.10        98\n",
            "           1       0.54      0.76      0.63       212\n",
            "           2       0.30      0.28      0.29       110\n",
            "\n",
            "    accuracy                           0.47       420\n",
            "   macro avg       0.39      0.37      0.34       420\n",
            "weighted avg       0.43      0.47      0.42       420\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple model\n",
        "\n",
        "input_shape = (window_size, X_resampled.shape[2]) # Use X_resampled to get the correct number of features\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=input_shape),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model1.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cksSVpUKppVs",
        "outputId": "5f481006-1cfc-4f7d-a731-4da93892f16d"
      },
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHhf6_BF-_zQ",
        "outputId": "1e532f3b-56c1-4fb4-ca9d-931d74e619d4"
      },
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 88ms/step - accuracy: 0.5134 - loss: 1.0405 - val_accuracy: 0.4619 - val_loss: 1.0562\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5424 - loss: 0.9937 - val_accuracy: 0.4595 - val_loss: 1.0570\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.5192 - loss: 1.0072 - val_accuracy: 0.4571 - val_loss: 1.0534\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.5617 - loss: 0.9554 - val_accuracy: 0.4500 - val_loss: 1.0522\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - accuracy: 0.5731 - loss: 0.9573 - val_accuracy: 0.4381 - val_loss: 1.0663\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 94ms/step - accuracy: 0.5672 - loss: 0.9383 - val_accuracy: 0.3881 - val_loss: 1.0956\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 105ms/step - accuracy: 0.5662 - loss: 0.9719 - val_accuracy: 0.4524 - val_loss: 1.0680\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.5668 - loss: 0.9530 - val_accuracy: 0.4500 - val_loss: 1.0675\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.5759 - loss: 0.9314 - val_accuracy: 0.3905 - val_loss: 1.0965\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.5666 - loss: 0.9370 - val_accuracy: 0.4167 - val_loss: 1.0844\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa2ff6e9820>"
            ]
          },
          "metadata": {},
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGYzil8c_Fb6",
        "outputId": "d5d87ac6-3c7b-441c-9d74-eb6749de4c76"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "[[  6  64  28]\n",
            " [  6 162  44]\n",
            " [  6  73  31]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.06      0.10        98\n",
            "           1       0.54      0.76      0.63       212\n",
            "           2       0.30      0.28      0.29       110\n",
            "\n",
            "    accuracy                           0.47       420\n",
            "   macro avg       0.39      0.37      0.34       420\n",
            "weighted avg       0.43      0.47      0.42       420\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Medium Volatility"
      ],
      "metadata": {
        "id": "P0dWJzUzqKIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "\n",
        "mdf = mdf.copy()\n",
        "mdf['Date'] = pd.to_datetime(mdf['Date'])\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Volatility_Regime', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "ZHQdC3IyqP8J"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = mdf.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year'])\n",
        "y = mdf['Direction']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "ZKWgnQ8mqxjv"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy arrays\n",
        "X_scaled = np.array(X_scaled)\n",
        "y = np.array(y)\n",
        "\n",
        "window_size = 50  # or any value you decide\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for i in range(window_size, len(X_scaled)):\n",
        "    X_seq.append(X_scaled[i - window_size:i])\n",
        "    y_seq.append(y[i])\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n",
        "\n",
        "print(f\"X_seq shape: {X_seq.shape}\")  # (samples, window_size, num_features)\n",
        "print(f\"y_seq shape: {y_seq.shape}\")  # (samples,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEwFicuDq0UK",
        "outputId": "f081bf6f-11ee-4b64-ddce-a8cddb91156e"
      },
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq shape: (1505, 50, 12)\n",
            "y_seq shape: (1505,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get corresponding dates for each sequence\n",
        "dates = mdf['Date'][window_size:].reset_index(drop=True) # Use hdf instead of df\n",
        "\n",
        "# Create boolean masks\n",
        "train_mask = dates < '2020-01-01'\n",
        "test_mask = (dates >= '2020-01-01') & (dates <= '2025-12-31')\n",
        "\n",
        "# Apply masks\n",
        "X_train = X_seq[train_mask.values]\n",
        "y_train = y_seq[train_mask.values]\n",
        "X_test = X_seq[test_mask.values]\n",
        "y_test = y_seq[test_mask.values]\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "print(\"Train range:\", dates[train_mask].min(), \"→\", dates[train_mask].max())\n",
        "print(\"Test range:\", dates[test_mask].min(), \"→\", dates[test_mask].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQvoQuK3q2sa",
        "outputId": "edf3fe3d-d982-4258-a39d-982785dd6bb4"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (1080, 50, 12) Test size: (425, 50, 12)\n",
            "Train range: 2010-09-07 00:00:00 → 2019-12-10 00:00:00\n",
            "Test range: 2020-01-07 00:00:00 → 2025-06-17 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: check class balance or visualize\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Train class distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(\"Test class distribution:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihbo8H9xq5kk",
        "outputId": "3fddccb4-083c-42bf-b75f-0fc4ae251eb0"
      },
      "execution_count": 289,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution: {np.int64(0): np.int64(241), np.int64(1): np.int64(621), np.int64(2): np.int64(218)}\n",
            "Test class distribution: {np.int64(0): np.int64(112), np.int64(1): np.int64(181), np.int64(2): np.int64(132)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#complex model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Bidirectional(\n",
        "        layers.LSTM(\n",
        "            64, return_sequences=True,\n",
        "            dropout=0.3, recurrent_dropout=0.2,\n",
        "            kernel_regularizer=regularizers.l2(1e-4),\n",
        "            recurrent_regularizer=regularizers.l2(1e-4)\n",
        "        ),\n",
        "        input_shape=(60, X_seq.shape[2])\n",
        "    ),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LSTM(\n",
        "        64,\n",
        "        dropout=0.3, recurrent_dropout=0.2,\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9TVknRTq8nI",
        "outputId": "65695061-c681-44c6-e6d1-70b00bad7bb0"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTDyLbrdrCh0",
        "outputId": "f5e25f92-9437-4987-891d-3abf46c6d30d"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 351ms/step - accuracy: 0.4470 - loss: 1.1131 - val_accuracy: 0.4259 - val_loss: 1.1144\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 175ms/step - accuracy: 0.5233 - loss: 1.0420 - val_accuracy: 0.4259 - val_loss: 1.1152\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.5039 - loss: 1.0721 - val_accuracy: 0.4235 - val_loss: 1.1060\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 246ms/step - accuracy: 0.5584 - loss: 1.0252 - val_accuracy: 0.4259 - val_loss: 1.1084\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.5713 - loss: 1.0079 - val_accuracy: 0.4259 - val_loss: 1.1063\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 177ms/step - accuracy: 0.5319 - loss: 1.0165 - val_accuracy: 0.4329 - val_loss: 1.1091\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 230ms/step - accuracy: 0.5783 - loss: 0.9967 - val_accuracy: 0.4259 - val_loss: 1.1034\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 226ms/step - accuracy: 0.5265 - loss: 1.0250 - val_accuracy: 0.4259 - val_loss: 1.1033\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 171ms/step - accuracy: 0.5621 - loss: 0.9885 - val_accuracy: 0.4306 - val_loss: 1.1009\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 165ms/step - accuracy: 0.5449 - loss: 1.0191 - val_accuracy: 0.4235 - val_loss: 1.1069\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa265d395b0>"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaI8n1pqrFJ9",
        "outputId": "750eb426-e0e4-434c-add6-d723669fbc9c"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step\n",
            "[[  7  91  14]\n",
            " [ 12 148  21]\n",
            " [ 13  94  25]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.06      0.10       112\n",
            "           1       0.44      0.82      0.58       181\n",
            "           2       0.42      0.19      0.26       132\n",
            "\n",
            "    accuracy                           0.42       425\n",
            "   macro avg       0.36      0.36      0.31       425\n",
            "weighted avg       0.38      0.42      0.35       425\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple model\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=input_shape),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model1.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8sCtN4ftrHkf",
        "outputId": "0007df74-bd9a-4ae8-baf1-e0053c44967e"
      },
      "execution_count": 293,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLQ8CZ0ErMDv",
        "outputId": "bb0480bc-f359-477d-f995-f68be0ba728d"
      },
      "execution_count": 294,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.4945 - loss: 1.0599 - val_accuracy: 0.4329 - val_loss: 1.0827\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.5158 - loss: 1.0223 - val_accuracy: 0.3929 - val_loss: 1.0854\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 71ms/step - accuracy: 0.5428 - loss: 0.9882 - val_accuracy: 0.4565 - val_loss: 1.0709\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 104ms/step - accuracy: 0.5830 - loss: 0.9453 - val_accuracy: 0.4282 - val_loss: 1.0783\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 101ms/step - accuracy: 0.5647 - loss: 0.9654 - val_accuracy: 0.4094 - val_loss: 1.0792\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5580 - loss: 0.9700 - val_accuracy: 0.4000 - val_loss: 1.1142\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5531 - loss: 0.9741 - val_accuracy: 0.4141 - val_loss: 1.0920\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.5664 - loss: 0.9437 - val_accuracy: 0.3788 - val_loss: 1.1157\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 70ms/step - accuracy: 0.6187 - loss: 0.9319 - val_accuracy: 0.3576 - val_loss: 1.1446\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 72ms/step - accuracy: 0.6101 - loss: 0.9065 - val_accuracy: 0.3318 - val_loss: 1.1416\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa2700a9940>"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model1.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4dhh1JErOhR",
        "outputId": "f5858c69-92ff-4973-a840-9d20f72f40b6"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m14/14\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step\n",
            "[[16 26 70]\n",
            " [42 56 83]\n",
            " [39 24 69]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      0.14      0.15       112\n",
            "           1       0.53      0.31      0.39       181\n",
            "           2       0.31      0.52      0.39       132\n",
            "\n",
            "    accuracy                           0.33       425\n",
            "   macro avg       0.33      0.32      0.31       425\n",
            "weighted avg       0.36      0.33      0.33       425\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Low Volatility"
      ],
      "metadata": {
        "id": "2e_tSgpHrR6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical variables\n",
        "\n",
        "ldf = ldf.copy()\n",
        "ldf['Date'] = pd.to_datetime(ldf['Date'])\n",
        "label_encoders = {}\n",
        "for col in ['EMA_Compare', 'Volatility_Regime', 'Direction']:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n"
      ],
      "metadata": {
        "id": "yegsEZW-rV5M"
      },
      "execution_count": 296,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = ldf.drop(columns=['Direction', 'Close_SOLARINDS.NS', 'Date', 'Year'])\n",
        "y = ldf['Direction']\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "0LIsrddxrZ_z"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to numpy arrays\n",
        "X_scaled = np.array(X_scaled)\n",
        "y = np.array(y)\n",
        "\n",
        "window_size = 50  # or any value you decide\n",
        "X_seq, y_seq = [], []\n",
        "\n",
        "for i in range(window_size, len(X_scaled)):\n",
        "    X_seq.append(X_scaled[i - window_size:i])\n",
        "    y_seq.append(y[i])\n",
        "\n",
        "X_seq = np.array(X_seq)\n",
        "y_seq = np.array(y_seq)\n",
        "\n",
        "print(f\"X_seq shape: {X_seq.shape}\")  # (samples, window_size, num_features)\n",
        "print(f\"y_seq shape: {y_seq.shape}\")  # (samples,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWJfHDh7rdpd",
        "outputId": "6fc3ae6d-89ae-44bc-8103-15d2e5e953e0"
      },
      "execution_count": 298,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_seq shape: (1116, 50, 12)\n",
            "y_seq shape: (1116,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get corresponding dates for each sequence\n",
        "dates = ldf['Date'][window_size:].reset_index(drop=True) # Use hdf instead of df\n",
        "\n",
        "# Create boolean masks\n",
        "train_mask = dates < '2020-01-01'\n",
        "test_mask = (dates >= '2020-01-01') & (dates <= '2025-12-31')\n",
        "\n",
        "# Apply masks\n",
        "X_train = X_seq[train_mask.values]\n",
        "y_train = y_seq[train_mask.values]\n",
        "X_test = X_seq[test_mask.values]\n",
        "y_test = y_seq[test_mask.values]\n",
        "\n",
        "print(\"Train size:\", X_train.shape, \"Test size:\", X_test.shape)\n",
        "print(\"Train range:\", dates[train_mask].min(), \"→\", dates[train_mask].max())\n",
        "print(\"Test range:\", dates[test_mask].min(), \"→\", dates[test_mask].max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmNij85Jrfrt",
        "outputId": "d3168d5e-9db9-4b84-da79-4e5b86b6828e"
      },
      "execution_count": 299,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: (533, 50, 12) Test size: (583, 50, 12)\n",
            "Train range: 2014-02-25 00:00:00 → 2019-12-31 00:00:00\n",
            "Test range: 2020-01-02 00:00:00 → 2025-10-30 00:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: check class balance or visualize\n",
        "import numpy as np\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "print(\"Train class distribution:\", dict(zip(unique, counts)))\n",
        "\n",
        "unique, counts = np.unique(y_test, return_counts=True)\n",
        "print(\"Test class distribution:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5bBJ4vYriwY",
        "outputId": "770183a5-8268-4698-f452-10705c3ef01a"
      },
      "execution_count": 300,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train class distribution: {np.int64(0): np.int64(90), np.int64(1): np.int64(330), np.int64(2): np.int64(113)}\n",
            "Test class distribution: {np.int64(0): np.int64(145), np.int64(1): np.int64(292), np.int64(2): np.int64(146)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#complex model\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "    layers.Bidirectional(\n",
        "        layers.LSTM(\n",
        "            64, return_sequences=True,\n",
        "            dropout=0.3, recurrent_dropout=0.2,\n",
        "            kernel_regularizer=regularizers.l2(1e-4),\n",
        "            recurrent_regularizer=regularizers.l2(1e-4)\n",
        "        ),\n",
        "        input_shape=(60, X_seq.shape[2])\n",
        "    ),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.LSTM(\n",
        "        64,\n",
        "        dropout=0.3, recurrent_dropout=0.2,\n",
        "        kernel_regularizer=regularizers.l2(1e-4)\n",
        "    ),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRm-fBxOrm7w",
        "outputId": "297a4481-9c7c-4057-d6b7-0c8ccbdc5389"
      },
      "execution_count": 301,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/bidirectional.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmq4WFBGrqLy",
        "outputId": "b6356a1f-b6d1-473e-fe72-487b3ebee874"
      },
      "execution_count": 302,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 306ms/step - accuracy: 0.3714 - loss: 1.1401 - val_accuracy: 0.5026 - val_loss: 1.0913\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 169ms/step - accuracy: 0.5253 - loss: 1.0424 - val_accuracy: 0.5009 - val_loss: 1.0863\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 204ms/step - accuracy: 0.5414 - loss: 1.0330 - val_accuracy: 0.5009 - val_loss: 1.0811\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 174ms/step - accuracy: 0.5181 - loss: 1.0688 - val_accuracy: 0.4906 - val_loss: 1.0844\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 176ms/step - accuracy: 0.5257 - loss: 1.0314 - val_accuracy: 0.4974 - val_loss: 1.0715\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 249ms/step - accuracy: 0.5551 - loss: 1.0313 - val_accuracy: 0.4648 - val_loss: 1.0792\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 188ms/step - accuracy: 0.5381 - loss: 1.0342 - val_accuracy: 0.4631 - val_loss: 1.0799\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 176ms/step - accuracy: 0.5260 - loss: 1.0386 - val_accuracy: 0.4717 - val_loss: 1.0744\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - accuracy: 0.5386 - loss: 1.0054 - val_accuracy: 0.4889 - val_loss: 1.0725\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 211ms/step - accuracy: 0.5412 - loss: 1.0317 - val_accuracy: 0.4854 - val_loss: 1.0720\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa25b9f8830>"
            ]
          },
          "metadata": {},
          "execution_count": 302
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_ndHUVirs2Z",
        "outputId": "f3a90223-7c62-4f4c-cc35-520858a5c223"
      },
      "execution_count": 303,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 97ms/step\n",
            "[[  9 136   0]\n",
            " [ 18 274   0]\n",
            " [ 14 132   0]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.06      0.10       145\n",
            "           1       0.51      0.94      0.66       292\n",
            "           2       0.00      0.00      0.00       146\n",
            "\n",
            "    accuracy                           0.49       583\n",
            "   macro avg       0.24      0.33      0.25       583\n",
            "weighted avg       0.31      0.49      0.35       583\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#simple model\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(128, input_shape=input_shape),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])\n",
        "\n",
        "#LSTM(batch_size, tiemsteps, features)\n",
        "#input_shape = (timesteps, features)\n",
        "#timesteps = number of datapoints = length of time series\n",
        "\n",
        "\n",
        "model1.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy', #because classes: 0, 1, 2: integer values\n",
        "        metrics=['accuracy']\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XErvRJr_rxR6",
        "outputId": "2c2751b3-52e4-4b15-ee3e-004702b98bf6"
      },
      "execution_count": 304,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/rnn/rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(\n",
        "    X_resampled, y_resampled,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=10, batch_size=32,\n",
        "    #callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yy0WYxjr0A9",
        "outputId": "cc0fd7ca-2e65-4d1d-a459-0299eddad064"
      },
      "execution_count": 305,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 92ms/step - accuracy: 0.5232 - loss: 1.0426 - val_accuracy: 0.4751 - val_loss: 1.0505\n",
            "Epoch 2/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5234 - loss: 1.0096 - val_accuracy: 0.4151 - val_loss: 1.0576\n",
            "Epoch 3/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 93ms/step - accuracy: 0.5389 - loss: 0.9878 - val_accuracy: 0.4305 - val_loss: 1.0885\n",
            "Epoch 4/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 109ms/step - accuracy: 0.5596 - loss: 0.9717 - val_accuracy: 0.4528 - val_loss: 1.0692\n",
            "Epoch 5/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 75ms/step - accuracy: 0.5647 - loss: 0.9522 - val_accuracy: 0.4443 - val_loss: 1.0860\n",
            "Epoch 6/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.5401 - loss: 0.9635 - val_accuracy: 0.4134 - val_loss: 1.1117\n",
            "Epoch 7/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - accuracy: 0.5751 - loss: 0.9380 - val_accuracy: 0.4340 - val_loss: 1.1203\n",
            "Epoch 8/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.5936 - loss: 0.9366 - val_accuracy: 0.4254 - val_loss: 1.1242\n",
            "Epoch 9/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.5928 - loss: 0.9407 - val_accuracy: 0.4065 - val_loss: 1.1257\n",
            "Epoch 10/10\n",
            "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 137ms/step - accuracy: 0.5943 - loss: 0.9354 - val_accuracy: 0.4220 - val_loss: 1.1118\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7fa259a272c0>"
            ]
          },
          "metadata": {},
          "execution_count": 305
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "y_pred = np.argmax(model1.predict(X_test), axis=1)\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAgfxAO8r2F0",
        "outputId": "6a8bc4b2-a789-41a9-9395-6417d957298e"
      },
      "execution_count": 306,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step\n",
            "[[ 26  53  66]\n",
            " [ 33 150 109]\n",
            " [ 22  54  70]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.18      0.23       145\n",
            "           1       0.58      0.51      0.55       292\n",
            "           2       0.29      0.48      0.36       146\n",
            "\n",
            "    accuracy                           0.42       583\n",
            "   macro avg       0.40      0.39      0.38       583\n",
            "weighted avg       0.44      0.42      0.42       583\n",
            "\n"
          ]
        }
      ]
    }
  ]
}